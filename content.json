{"posts":[{"title":"碎页城","text":"记录一些碎片 .gist-meta { display: none; } div.gist-meta::after { content: \"yu still\"; font-style: italic; visibility: visible; } .gist .markdown-body { font-size: 15.4px; } var divs = document.querySelectorAll('.gist-meta'); divs.forEach(function(div) { for (var i = 0; i < div.childNodes.length; i++) { var node = div.childNodes[i]; if (node.nodeType === Node.TEXT_NODE) { node.nodeValue = node.nodeValue.replace('hosted with ❤ by', '❤ A-'); } else if (node.nodeType === Node.ELEMENT_NODE && node.nodeName === 'A') { /*if (node.textContent.includes('.')) { node.textContent = ''; } else { node.textContent = node.textContent.replace('GitHub', ''); node.textContent = node.textContent.replace('view raw', ''); }*/ node.textContent = ''; } } });","link":"/2024/04/14/wanderer/2024-04-14-%E7%A2%8E%E5%8F%B6%E5%9F%8E/"},{"title":"SWS3004-Lecture 4: Cloud Software Development and Deployment","text":"L4 is about Cloud Software Development and Deployment. Outline Cloud Software Development Learning Objectives SaaS is Different from Traditional Software Different Perspectives of SaaS Development SaaS with Self-managed Infrastructure and Self-managed Platform SaaS with Self-managed Platform SaaS with Self-managed Infrastructure SaaS with Cloud-enabled IaaS and SaaS Summary Cloud Software Development Power of cloud computing – exploits higher service-level abstraction, PaaS and SaaS, to reduce the time and cost of software development SaaS changes the way software is delivered usage-based billing, high scalability, ease of access, automated updates PaaS changes the way the (SaaS) software is developed automates the process of deployment, testing and scaling to reduce manual work and cost of application development Learning Objectives Understand how SaaS applications are different from traditional software/application Understand the different perspectives of SaaS software development SaaS is Different from Traditional Software Pay-per-use - provides web access to commercial software on pay-as you-use vs traditional pay the full license fee Zero infrastructure – customers need not install the software (SaaS developed, deployed and managed by service provider) vs ASP (application service provider) owns and manages dedicated infrastructure for each customers Reduced business cost – 1-many - same SaaS application shared by multiple customers (multi-tenants) vs traditional 1-1 end-users and software relationship Automated updates – Updates performed by service providers (SaaS) not by users (traditional) Suitability of SaaS Not suitable Real-time processing where fast processing of data is needed Organization’s data is more confidential and data localization is needed When on-premise applications fulfil organization’s needs Suitable Consumers require on-demand software rather than full term/licensing-based software Start-up company that cannot invest in buying licensed software Applications with unpredictable and dynamic load Different Perspectives Of SaaS Development 2 key challenges choosing correct multitenancy level(s) - multitenancy can be achieved at different levels such as infrastructure, platform and application governance and security over user data four perspectives SaaS with Self-managed Infrastructure and Self-managed Platform SaaS with Self-managed Platform SaaS with Self-managed Infrastructure SaaS with Cloud-enabled IaaS and PaaS SaaS with Self-managed Infrastructure and Self-managed Platform SaaS with Self-managed Platform SaaS with Self-managed Infrastructure SaaS with Cloud-enabled IaaS and PaaS Summary cloud software development SaaS changes the way software is delivered SaaS is different from traditional software SaaS development: from self-managed to cloud enabled IaaS and/or PaaS cloud-enabled platform services (with advanced capabilities around artificial intelligence (AI), data analytics, blockchain, IoT, …) + server-less computing changes the development, deployment and cost of more complex software applications","link":"/2023/08/10/SWS-3004/SWS3004-Lecture-4/"},{"title":"SWS3004-Lecture 3: Big Data Architecture and Patterns","text":"L3 is about Big Data Architecture and Patterns. Overview Big Data and Big Data Architectures Platforms MapReduce, Hadoop, and HDFS Spark Cloud Dataflow and Beam Big Data and Big Data Architectures Big Data Big Data Architectures Big Data Platform and Applications Multicore/Cluster era → parallel and distributed applications Parallelism Task-parallel Data-parallel Batch (MapReduce, Spark, Cloud Dataflow) Stream (Spark Streaming, Flink, Cloud Dataflow) Mixed System resource demand Compute-intensive Data-intensive Mixed Platforms MapReduce, Hadoop, and HDFS MapReduce Programming Model Supports arbitrarily divisible workload Supports distributed computing on large data sets on multiple machines (clusters, public or private clouds, …) How large an amount of work? Web-scale data on the order of 100s of GBs to TBs or PBs Input data set will not likely fit on a single computer’s hard drive Distributed file system (e.g., Google File System- GFS) is typically required Inspired by the map and reduce functions in functional programming languages Key idea Split data into blocks and assign each block to an instance/process for parallel execution Merge partial results produced by individual instances after all instances completed execution Transform a set of input &lt;key, value&gt; pairs into a set of output &lt;key, value&gt; pairs SPMD (Same Program Multiple Data) - a master instance partitions the data and gathers the partial results Structure of a MapReduce Program Read (a lot of) data MAP (extract data you need from each record) Shuffle and Sort data REDUCE (aggregate, summarize, filter, transform extracted data) Write the results Hadoop Google MapReduce - closed source developed by Google (2004) to process large amounts of raw data Hadoop MapReduce – open source developed by Apache + Yahoo (Java programming language) – a popular implementation of MapReduce Presents MapReduce as an analytics engine with a distributed storage layer referred to as Hadoop Distributed File System (HDFS); HDFS mimics Google File System (GFS) Example: Amazon Elastic MapReduce creates a Hadoop cluster and handles data transfers between Amazon EC2 (computation) and Amazon S3 (storage) Spark Apache Spark Distributed processing framework and programming model for big data workloads Utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size It provides development APIs in Java, Scala, Python and R, and supports code reuse across multiple workloads: Batch processing Machine Learning Interactive queries Real-time analytics Spark RDD Main abstraction: resilient distributed dataset (RDD) - a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDD - created from the file system or through operations (transformations) Option to persist an RDD in memory or on disk to be reused efficiently across parallel operations RDD used for fault tolerance: recover from node failures Cloud Dataflow and Beam Google Cloud Dataflow “The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing\" reference Apache Beam Parallel computing framework for data processing Multiple SDKs (Java, Python, Go, Scala) Unified data model (PCollection): batch (bounded) streaming (unbounded) Pipelines containing data processing operations (PTransform) Source transforms Processing and Conversion transforms (Count, Sum, Map, GroupBy) Outputting Transforms Used defined transforms Run on multiple execution engines called Runners DirectRunner (Local) Google Dataflow Apache Spark Hadoop Apache Flink","link":"/2023/08/08/SWS-3004/SWS3004-Lecture-3/"},{"title":"SWS3004-Lecture 2: Applications and Paradigms","text":"L2 is about Application and Paradigms of Cloud Computing. Outline Cloud Applications Common Features Applications Challenges in Developing Applications Architectural Styles for Cloud Applications Cloud Application Development Models Characteristics of Cloud Service Models Examples of Setting up a Blog Key Terms Divisible workload Performance isolation Web application architecture layers Application development models (IaaS, PaaS, SaaS) Cloud Applications Common Features of Cloud Providers Basic and higher - level services (IaaS -&gt; SaaS). Compute and storage resources - virtual servers (Linux and Windows) and object store. Deployment management services - load balancer, auto scaling, message queueing, monitoring, …. User interface - graphical user interface, command-line interface. Cloud Applications Focus mainly on enterprise computing. Ideally, an application can partition its workload into n segments and spawn n instances, and the execution time reduced by a factor close to n. (MapReduce) Key Challenges cloud consumer: scale application to accommodate a dynamic load, recover after a system failure, efficiently support checkpoint/restart cloud provider: manage a large number of systems (cloud consumer applications), provides quality of service guarantee Ideal Applications web services database services search services machine learning with massive-scale models Unlikely to perform well applications with a complex workflow and multiple dependencies such as high-performance computing applications with intensive communication among concurrent instances workload cannot be arbitrarily partitioned Challenges in Developing Cloud Applications Performance isolation – how to ensure that customer performance is not affected by other users? Reliability - major concern, server failures expected when a large number of servers cooperate to compute. Cloud infrastructure exhibits latency and bandwidth fluctuations which affect the application performance. Performance considerations limit the amount of data logging (identify unexpected results, errors, monitor application performance). Architectural Styles for Cloud Applications Reliance on Internet and web technology (high accessibility, web browser universality, ease of web-based service development). Cloud services use web technology as both the implementation medium and management interface. 2 basic components of the web are web browser client and web server, i.e., based on client-server architecture. “as-a-Service” Cloud Delivery Models Simple Object Access Protocol (SOAP) : Application protocol for web applications; defines a common web service messaging format for request and response message exchanges; based on the XML, uses TCP or UDP transport protocols. Representational State Transfer (REST): software architecture for distributed hypermedia systems. Supports client communication with stateless servers, platform and language independent, supports data caching, and can be used in the presence of firewalls. Rest is better in simplicity, flexibility, efficiency and scalability, which is suitable for web applications. SOAP is suitable for applications requiring higher security, transaction handling, and reliability, particularly in enterprise-level applications. Cloud Application Development Models Characteristics of Cloud Service Models Infrastructure as a Service (for IT architects) Web access to resources Centralized physical resource management Elastic services and dynamic scaling Shared infrastructure across multiple users Preconfigured VMs Metered services Platform as a Service (for developers) All in one – same IDE to develop, test, deploy, host and maintain applications Web access to development platforms Offline access for developers Built-in scalability Collaborative platform for developers Diverse client tools Software as a Service (for end users) Multi-tenanted applications Web access Centralized management of SaaS services Multi-device support Scalability under varying loads High availability API integration with other software Examples of Setting up a Blog Bonus Track: Function as a Service","link":"/2023/08/03/SWS-3004/SWS3004-Lecture-2/"},{"title":"SWS3004-Lecture 1: Concepts and Models","text":"L1 is about Concepts and Models of Cloud Computing. Outline NIST Definition Cloud Characteristics Cloud Service(Delivery) Models Conceptual Reference Architecture Cloud Deployment Models Summary Key Terms Elasticity On-demand self service Pay-per-use (measured service) Multi-tenancy (location independent resource pooling) Cloud service (delivery) models Cloud deployment models Cloud actors Definition Cloud means \"smooth\" to access, control and measure. It has five essential characteristics, four deployment models and three service models. Cloud Characteristics On-demand self-service through a service portal With cloud computing, you can provision computing services, like server time and network storage, automatically. You won’t need to interact with the service provider. Cloud customers can access their cloud accounts through a web self-service portal to view their cloud services, monitor their usage, and provision and de-provision services. Broad network access (ubiquitous access) Users can access cloud services anytime and anywhere through a terminal device with network connection. Latency and bandwidth both count because they affect the quality of service. Location-independent resource pooling (multi-tenancy) Computing resources are gathered together as pools, like CPU pools, memory pools, etc. With resource pooling, multiple customers can share physical resources using a multi-tenancy model. This model allows customers to share the same applications or infrastructure while maintaining privacy and security. It's a fantastic characteristic of cloud, which abstracts and subdivides physical resources. Rapid elasticity – time to market / fast deployment Cloud services can be elastically provisioned and released, sometimes automatically, so customers can scale quickly based on demand. With rapid and unlimited elasticity of cloud service, you don't need to buy hardware but use cloud resources to satisfy your demand. Measured service (pay-per-use) In cloud systems, a metering capability optimizes resource usage at a level of abstraction appropriate to the type of service. For example, you can use a measured service for storage, processing, bandwidth, and users. Payment is based on actual consumption by the customer via a pay-for-what-you-use model. Monitoring, controlling, and reporting resource use creates a transparent experience for both consumers and providers of the service. Cloud Service(Delivery) Models There are three main models: SaaS, PaaS and IaaS. As for \"steak\" service, IaaS is like providing a kitchen with some pots, PaaS provides raw beef and pepper additionally, and SaaS provides a plate of steak. More convenience, but less space to select. Conceptual Reference Architecture Actor Roles Cloud Consumer - maintains a business relationship with, and uses service from Cloud Providers. Cloud Provider – offers a cloud service to cloud consumers. Cloud Auditor - conducts independent assessment of cloud services, system operations, performance and security of the cloud implementation. Cloud Broker - manages the use, performance and delivery of cloud services, and negotiates relationships between Cloud Providers and Cloud Consumers. Cloud Carrier - provides connectivity and transport of cloud services from Cloud Providers to Cloud Consumers. CLOUD DEPLOYMENT MODELS Private cloud solely for used by an organization for enterprises/corporations with large scale IT Public cloud available to general public, i.e., shared by all consumers open market for on demand computing and IT resources concerns: limited SLA, reliability, availability, security, trust Community cloud shared by several organizations and supporting a specific community Hybrid (federated) cloud two or more public and private clouds that interoperate extends private cloud(s) to include a shared public cloud","link":"/2023/07/28/SWS-3004/SWS3004-Lecture-1/"},{"title":"SWS3004-Lab Exercise 2","text":"This Lab is about Hadoop and Spark, using AWS EMR and S3. Exercise 2.1 Run and compare the execution time of WordCount on Wikipidia’s dump with both Hadoop MapReduce and Spark. You can use either IaaS or PaaS, but make sure you use the same type of setup for both Hadoop and Spark (e.g., if you use EMR for Hadoop MapReduce, then use EMR for Spark also). You have to use the provided input of size 12 GB. Is there any difference in the programming model and ease of programming? Is there any difference in performance? Please explain it in maximum 3 paragraphs. You can include up to 2 performance plots. Input dataset address on AWS S3: s3://sws3004-2023/input/enwiki-12GB.xml You must use this input dataset for both Hadoop MapReduce and Spark. (Tip: use the entire address s3://sws3004-2023/input/enwiki-12GB.xml as parameter to your MapReduce job) Part 1. Hadoop Mapreduce First I create a S3 bucket and upload files: Then I create an EMR cluster, select the S3 bucket created in the previous step as my S3 folder, and select m4.large and default 3 instances as the instance configuration. Now add a step, using WordCount.jar to process the input from s3://sws3004-2023/input/enwiki-12GB.xml. In my S3 bucket, I can check the output when the step finished. Hadoop MapReduce performance: The process takes 42 minutes totally. Part 2. Spark I clear the S3 bucket and upload the files of Spark: Then create a new EMR for Spark, and add a step. Spark performance: The process takes 16 minutes totally. It seems that Spark is better than Hadoop in performance. The reason is Hadoop uses disk to store data while Spark uses memory to store data, which can reduce the I/O time. Also, MapReduce requires a lot of time to sort during Shuffle, and sorting seems inevitable in MapReduce's Shuffle. When Spark is in Shuffle, sorting is only required for some situations, which is faster. Is there any difference in the programming model and ease of programming? Hadoop MapReduce: Hadoop MapReduce is a programming model designed for distributed data processing on large clusters of commodity hardware. The MapReduce programming model has two steps to process our data: Map and Reduce. Map: In the Map phase, the input data is divided into splits, and each split is processed independently by multiple mapper tasks in parallel. The mapper tasks extract key-value pairs from the input data and emit intermediate key-value pairs. Shuffle and Sort: The intermediate key-value pairs emitted by the mappers are shuffled and sorted based on the keys. This step ensures that all values for the same key are grouped together and sent to the same reducer task. Reduce: In the Reduce phase, the sorted and shuffled intermediate data is processed by reducer tasks. Each reducer task processes a subset of the intermediate data, grouped by keys. The reducer tasks aggregate the values associated with each key and produce the final output. Spark: Spark is a better distributed data processing engine, which extends the MapReduce model and offers more versatility and performance improvements. Spark introduces the concept of Resilient Distributed Datasets (RDDs), which are the fundamental data abstraction in Spark. RDDs are distributed collections of data that can be processed in parallel. Spark provides a more general programming model compared to Hadoop MapReduce. It supports not only Map and Reduce operations but also various other transformations and actions on RDDs, such as filter, join, groupByKey, reduceByKey and so on. Additionally, Spark offers specialized libraries like Spark SQL for structured data processing, Spark Streaming for real-time data streaming, and MLlib for machine learning tasks. Difference in Programming Model and Ease of Programming: I found a comparison form in Lecture slides: Programming Model: Hadoop MapReduce has a more rigid programming model, where data is processed in two distinct phases (Map and Reduce), and users need to explicitly handle intermediate data shuffle and sort. Spark provides a more flexible and expressive programming model with RDDs, allowing users to perform complex operations on distributed data through a wide range of transformations and actions. Ease of Programming: Spark generally offers better ease of programming due to its high-level APIs and expressive transformations and actions on RDDs. It simplifies the development of distributed data processing applications, and its concise syntax often leads to shorter and more readable code compared to Hadoop MapReduce. Hadoop MapReduce, being more low-level, might require developers to write additional code for tasks like intermediate data serialization and deserialization, which can make the development process more cumbersome. In summary, Spark provides a more powerful and user-friendly programming model compared to Hadoop MapReduce. Spark's RDDs and higher-level APIs make it easier for developers to write distributed data processing applications, leading to faster development cycles and more efficient data processing. Exercise 2.2 Write and run on AWS EMR a MapReduce program that computes the total number of followers and followees for each user in a Twitter dataset. The dataset is provided to you in the file twitter_combined.txt taken from http://snap.stanford.edu/data/egonets-Twitter.html. Each line of this file contains two user ids A and B meaning “User A follows User B”. For example, the first line is “214328887 34428380” and it means that “User 214328887 follows User 34428380”. My code is below: 123456789101112131415161718192021222324252627282930// here is the mappublic static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { public void map(LongWritable key, Text texts, Context context) throws IOException, InterruptedException { String[] users = texts.toString().split(\" \"); // split as user0 user1 IntWritable followers = new IntWritable(-1); // negative IntWritable follows = new IntWritable(1); // positive context.write(new Text(users[1]), followers); context.write(new Text(users[0]), follows); }}// here is the reducepublic static class Reduce extends Reducer&lt;Text, IntWritable, Text, Text&gt; { public void reduce(Text texts, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int follows = 0; int followers = 0; for (IntWritable text : texts) { if (text.get() &gt; 0) { follows += text.get(); } else { followers -= text.get(); } } // output the result for each user context.write(texts, new Text(String.format(\"Followers %d\", inDegree))); context.write(texts, new Text(String.format(\"Follows %d\", outDegree))); }} Then we create a S3 and upload the files: Now we can add a step in EMR and view the final output: So User 214328887 has 628 followers, and follows 951 users.","link":"/2023/07/28/SWS-3004/AWS-Lab2/"},{"title":"SWS3004-Lab Exercise 1","text":"This Lab is based on Amazon Web Services(AWS)，including EC2, Lambda, SQS and CloudWatch. Exercise 1.1 Passwordless SSH access between two EC2 instances Create two (2) AWS EC2 instances (virtual machines) with a Linux-based operating system(e.g., Ubuntu) and set up passwordless SSH access among them. Let’s suppose we name those 2 instances A and B. Passwordless SSH means that we can SSH into instance A from instance B and vice versa without being asked for a password. How do you do that? Please explain it in 1-2 paragraphs. [3 marks] Firstly, I create two EC2 instances(serverA and serverB) with a Ubuntu-20.04 OS. The details are shown in Figure 1 and Figure 2 below. The IPv4 of serverA is 54.174.141.190, and the IPv4 of server B is 44.207.230.223. Let's start with password-less access from A to B. First, type the command ssh-keygen -t rsa in instance A. This command will generate a pair of public/private keys in the ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa, shown in Figure 3. I met a trouble here: I use command ssh-copy-id -i ~/.ssh/id_rsa.pub ubuntu@44.207.230.223 to copy the public key of A to the authorized_keys of B, but it shows Permission denied. The reason for this error is this command will overwrite the file authorized_keys of B, but this file is already exists so this command was denied. Instead, I use cat ~/.ssh/id_rsa.pub in instance A to display my public key, and use echo \"MY_PUBLIC_KEY\" &gt;&gt; ~/.ssh/authorized_keys in instance B to paste the key. Now type ssh ubuntu@44.207.230.223 in instance A, we can access instance B successfully, without any password! View relevant screenshot in Figure 4. Symmetrically, to make instance B to access to A without any password, just generate ssh-key pair in instance B, and follow the steps above once again. In Figure 5, both A and B successfully access to each other, without password. Finally, I summarized the theory of password-less SSH in figure 6. Password-less SSH using a different port Similar to Exercise 1.1., create two (2) AWS EC2 instances (virtual machines) with a Linux based operating system (e.g., Ubuntu) and setup passwordless SSH access among them but this time use a different port for the SSH server (change the default port 22 to port 2222). Do you need to make any other modifications to your EC2 instances? [4 marks] Let's continue on the basis of Exercise 1.1. We have achieved password-less SSH access in the default port 22, and now we need to achieve it in port 2222. First, I edit the security groups of instance A and B, adding a new rule to allow 2222 port, just like Figure 7 below. Then I add Port 2222 in /etc/ssh/sshd_config, shown in Figure 8. However, I failed for the first time. How could be? The key is not to forget to restart the service. Type sudo service ssh restart to restart service, and then do password-less SSH access. In figure 9, Instance A and B can password-less access to each other using Port 2222. Exercise 1.2 Start: Hello World! Start by creating and running a Python AWS hello world using AWS Management Console, as shown during Lecture 2. Take screenshots of your Lambda function, test event and the log output to show that the program runs successfully. Pay attention to setting the role of the Lambda function to “LabRole”. [2 marks] I create and run a Python AWS hello world using AWS Management Console, the screenshots of lambda function, test event and the log output are shown in Figure 10/11/12. Create an AWS SQS Create an AWS Simple Queue Service (SQS) Queue and take a screenshot of the created queue. After creating the queue, note down its Amazon Resource Name (ARN). [2 marks] Here we create a AWS SQS Queue shown in Figure 13. The ARN is arn:aws:sqs:us-east-1:368136098362:Queue1 Change the code of Lambda Function Change the code of your Lambda function such that it returns the received message from SQS. You are allowed to search on the Internet for how to do that. Please include the code in your submission(report). Next, add the created queue (identified by its ARN) as trigger for the Lambda function. Take a screenshot of the “Function overview Info” section of your Lambda function. [2 marks] Now we need to change the code of the Lambda function such that it returns the received message from SQS. The code is shown in the block below. 12345678import jsonprint('Loading function')def lambda_handler(event, context): print('Received message: %s' % event['Records'][0]['body']) message = event['Records'][0]['body'] return message Then we need to add the created queue (identified by its ARN) as trigger for the Lambda function. There are two ways here: The first way is add the trigger in AWS Interface, shown in Figure 14. The second way is to use command line interface, shown in Figure 15. Two queues are successfully added as trigger, shown in Figure 16. The Function Overview is shown in Figure 17. Send and Receive Message In the SQS dashboard of your queue, click “Send and receive message”, then send a message with the body “Hello from SWS3004!”. Message Group ID and Message deduplication ID can be set to 0. Press “Send message”. Next, go to CloudWatch -&gt; Logs -&gt; Log groups and find the logs for your Lambda function. Click on the relevant log stream (e.g., the latest). There should be a message “Hello from SWS3004!” somewhere in this log stream. Take a screenshot and include it in the report. [2 marks] We send a message hello:-) from my SQS queue1, and finds the logs for my function hello, shown as Figure 18 and Figure 19.","link":"/2023/07/15/SWS-3004/AWS-Lab1/"}],"tags":[{"name":"Cloud","slug":"Cloud","link":"/tags/Cloud/"},{"name":"Lab","slug":"Lab","link":"/tags/Lab/"},{"name":"碎片","slug":"碎片","link":"/tags/%E7%A2%8E%E7%89%87/"}],"categories":[{"name":"云开发","slug":"云开发","link":"/categories/%E4%BA%91%E5%BC%80%E5%8F%91/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"}],"pages":[{"title":"","text":"关于本博客 写作对我来说是一件爱恨交织的事情。我喜欢将自己的所思所感诉诸文字，却又往往囿于斟词酌句和重构文章结构中无法自拔。但我想这件事情总归是值得坚持的，它可以让我更加严肃的去审视、思考并总结，以安静的文字沉淀下来。这是我想要的生活方式吧，见字如面。 我的博客主要分为以下三个部分： 成于思：严肃思考 勤于学：知识学习 敏于行：工程实践 歌于途：生活随感 关于我 北京邮电大学计算机学院本科生 目前兴趣在全栈开发 共产主义者","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"life","text":"","link":"/life/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"","text":"勤于学 云计算（Cloud Computing with Big Data) NUS 暑校课程，包括 4 个 Lecture 和 2 个 Lab Lecture Note Concepts and Models Applications and Paradigms Big Data Architecture and Patterns Cloud Software Development and Deployment Lab Report Lab Exercise 1 Lab Exercise 2","link":"/study/index.html"},{"title":"","text":"敏于行：工程实践 虎符系列 “虎符”是徐鹏老师的实验室项目，我担任一些开发任务。 虎符前端工程实践1 虎符前端工程实践2 虎符前端工程实践3","link":"/tech/index.html"},{"title":"","text":"","link":"/think/index.html"}]}