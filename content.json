{"posts":[{"title":"MySQL 基础","text":"MySQL 基础用法笔记 MySQL 启动与连接 MySQL 安装完成后在系统启动时会自动启动，当然也可以手动启动停止 123net start mysqlnet stop mysql 命令行连接： 1mysql [-h 127.0.0.1] [-P 3306] -u root -p 参数（\"[ x ]\" 代表可选参数）： -h : MySQL服务所在的主机IP -P : MySQL服务端口号， 默认3306 -u : MySQL数据库用户名 -p ： MySQL数据库用户名对应的密码 SQL 语句 JetBrains 家的 DataGrip 支持图形化界面操作数据库或者写 SQL 语句，比命令行方便很多。我爱 JetBrains。 Data Definition Language (DDL) Data Definition Language，数据定义语言，用来定义数据库对象(数据库，表，字段) 。 数据库操作 查询所有数据库 123456show databases;-- 这是注释# 这也是注释/* 这也是注释*/ 查询当前数据库 1select database(); 创建数据库 1create database [ if not exists ] DB_NAME [ default charset 字符集 ] [ collate 排序规则 ]; 删除数据库 1drop database [ if exists ] DB_NAME; 切换数据库 1use DB_NAME; 表操作 查询创建 查询当前数据库所有表 1show tables; 查看指定表结构 1desc TABLE_NAME; 查看指定表的建表语句 1show create table TABLE_NAME; 创建表结构 1234567create table TABLE_NAME ( NAME1 TYPE1 [ comment 'comment' ], NAME2 TYPE2 [ comment 'comment' ], NAME3 TYPE3 [ comment 'comment' ], ...... NAMEn TYPEn [ comment 'comment' ]) [comment 'comment']; 数据类型 修改 添加字段 1alter table TABLE_NAME add 字段名 类型 [ comment 'comment' ] [ 约束 ]; 修改数据类型 1alter table TABLE_NAME modify 字段名 新数据类型; 删除字段 1alter table drop 字段名; 修改表名 1alter table 表名 rename to 新表名; 删除 删除表 1drop table [ if exists ] TABLE_NAME; 删除指定表并重新创建 1truncate table 表名; Data Manipulation Language (DML) 给指定字段添加数据 1insert into 表名 (字段名1, 字段名2, ...) values (值1, 值2, ...), (值1, 值2, ...); 给全部字段添加数据 1insert into 表名 values (值1, 值2, ...), (值1, 值2, ...); 修改数据 1update 表名 set 字段名1 = 值1, 字段名2 = 值2, ... [where 条件]; 删除数据 1delete from TABLE_NAME [ where 条件 ]; Data Query Language (DQL) 语法结构： 1234567891011121314select 字段列表from 表名列表where 条件列表group by 分组字段列表having 分组后条件列表order by 排序字段列表limit 分页参数 基础查询 查询多个字段 1select 字段1, 字段2, ... from TABLE_NAME; 12select * from TABLE_NAME;-- 不推荐 字段设置别名 1select 字段1 [ as NICKNAME1 ], 字段2 [ as NICKNAME2 ] ... from TABLE_NAME; 去除重复记录 1select distinct 字段列表 from TABLE_NAME; 条件查询 1select ... from TABLE_NAME where 条件列表; 聚合函数 将一列数据作为一个整体进行纵向计算，如 count max min avg sum 1select 聚合函数(字段列表) from 表名; 分组查询 1select 字段列表 from [ where ... ] group by 分组字段名 [ having 分组后字段过滤条件 ]; where &amp; having 执行时机不同，where 是分组之前进行过滤，不满足 where 条件，不参与分组；而 having 是分组之后对结果进行过滤 判断条件不同，where 不能对聚合函数进行判断，而 having 可以 分组之后查询字段一般为聚合函数和分组字段，其他字段毫无意义 eg. 查询年龄小于 45 岁的员工并根据工作地址分组，获取员工数量不少于 3 的工作地址 1select workaddress, count(*) address_count from emp where age &lt; 45 group by workaddress having address_count &gt;= 3; 排序查询 1select 字段列表 from 表名 order by 字段1 排序方式1, 字段2 排序方式2; ASC：升序（默认） DESC：降序 分页查询 1select 字段列表 from 表名 limit 起始索引, 查询记录数; 起始索引从0开始，起始索引 = （查询页码 - 1）* 每页显示记录数 如果查询的是第一页数据，起始索引可以省略，比如直接简写为 limit 10 执行顺序 Data Control Language (DCL) 管理用户 查询用户 1select * from mysql.user; 创建用户 1create user 'USER_NAME'@'HOST_NAME' identified by 'PASSWORD'; 修改用户密码 1alter user 'USER_NAME'@'HOST_NAME' identified with mysql_native_password BY 'NEW_PASSWORD'; 删除用户 1drop user 'USER_NAME'@'HOST_NAME'; 权限控制 查询权限 1show grants for 'USER_NAME'@'HOST_NAME'; 授予权限 1grant 权限列表 on DB_NAME.TABLE_NAME to 'USER_NAME'@'HOST_NAME'; 撤销权限 1revoke 权限列表 on DB_NAME.TABLE_NAME from 'USER_NAME'@'HOST_NAME'; 多个权限之间，使用逗号分隔 授权时， 数据库名和表名可以使用 * 进行通配，代表所有。","link":"/2023/09/02/DBS/MySQL1/"},{"title":"虎符前端工程实践3","text":"下拉框 删除顶部命名空间的显示 将 &lt;span class=\"base\"&gt;{{ $t('namespace') }}：{{ projectDetail.namespace }}&lt;/span&gt;&lt;br&gt; 替换成 &lt;br&gt; 命名空间下拉框 仿照 newContainer/index.vue 部分代码实现下拉框 1234567891011121314151617181920212223&lt;template&gt;... &lt;el-form-item :label=\"$t('namespace')\" prop=\"namespace\"&gt; &lt;el-select v-model=\"ruleForm.namespace\" :placeholder=\"$t('please')+$t('choose')+$t('namespace')\" style=\"width: 100%\"&gt; &lt;el-option v-for=\"ns in namespaceOptions\" :label=\"ns\" :value=\"ns\"&gt; &lt;/el-option&gt; &lt;/el-select&gt; &lt;/el-form-item&gt;...&lt;/template&gt;&lt;script&gt; methods: { fetchData() { getNSList().then(response =&gt; { this.namespaceOptions = response.data.nsList }) } }&lt;/script&gt; 国际化 采用 $t('namespace')，通过 Vue I18n 库实现国际化 修改应用命名空间的时候，不显示“default”(同时”应用管理/容器应用/新建容器”页面的下拉框也不显示“default”，即两个页面都要做此操作) namespaceOptions 是通过 getNSList() 获取数据的，后端是在 ResouceController 的 getNamespaces 继续调数据的，所以从根源上去掉 default 好像不太现实。前端有两种办法 v-if=\"ns !== 'default'，在下拉列表的时候去掉 this.namespaceOptions = response.data.nsList.filter(ns =&gt; ns !== 'default') 在请求列表数据后筛掉，我采取这个办法 只有在容器在”STOP”状态允许更改命名空间 加入 :disabled=\"projectStatus!=='STOP' \"","link":"/2023/09/01/hufu/%E8%99%8E%E7%AC%A6%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B53/"},{"title":"《Database System Concept》C2 -- Relational Model","text":"A brief introduction to relational model. Outline Structure of Relational Databases Database Schema Keys Schema Diagrams Relational Query Languages The Relational Algebra Structure of Relational Databases","link":"/2023/08/29/DBS/DBS_2/"},{"title":"《Database System Concept》C1 -- Introduction","text":"A brief introduction about basic principles of databases. Outline Database System Applications Purpose of Database Systems View of Data Database Languages Database Design Database Engine Database Application and Architecture Database Users and Administrators Database System Applications Databases have touched all aspects of our lives, a modern database system is a complex software system whose task is to manage a large, complex collection of data with various forms and structures. Concepts: DB, DBMS, DBS, DBAS Database (DB) A collection of interrelated data Stored in systems as files Database management system (DBMS), consisting of DB, or a collection of interrelated data A set of programs to access the data in DB Database system (DBS) Having the same definition as DBMS in the textbook The term DBS and DBMS are used interchangeably in this book Purpose of Database Systems Atomicity of updates Concurrent access by multiple users Security problems View of Data A database system is a collection of interrelated data and a set of programs that allow users to access and modify these data, a major purpose of a database system is to provide users with an abstracted view of the data, which means hiding some details about data storage and maintenance. Level of Abstraction Physical Level Describes how a record (e.g., instructor) is stored Logical Level Implementation/DB-oriented Describes data stored in database, and the relationships among the data View Level Top level abstraction, which describes a part of the whole database Application programs hide details of data types, can also hide information for security purposes Models, Schemas, Instances Data modeling, in line with data models, produces data schemas, including the logical and physical schema, which describe the organization of data in database. Logical Schema – the overall logical structure of the database Physical schema – the overall physical structure of the database DB Instance – the actual content of the database at a particular point in time Model vs Schemas vs Instance Similar to types, variables and values of variables in programming languages, like R={&lt;a1, a2, …., an&gt; } --&gt; instructor=&lt;ID, name, dept_name, salary&gt; --&gt; &lt;2222, Ayu, Physics, 95000&gt; Data Independence Independence between data uses/applications and data in DBS Logical data independence When DBS’s logical schemas at logical level (DBMS-oriented) change, application-oriented external schema or application programs do not change Physical data independence Application programs do not depend on DBS’s physical schemas Database Migration Change/replace DBMS platform, from Oracle to openGauss, avoiding changes of logical schemas and application programs and maintaining data independence, as much as possible Database Languages Database Definition Language Specification notation for defining the database schema DDL compiler generates a set of table templates stored in a data dictionary Data dictionary contains metadata (i.e., data about data) Database schema Integrity constraints Primary key (ID uniquely identifies instructors) Authorization (Who can access what) Database Manipulation Language DML, also known as query language, is a language for accessing and updating the data organized by the appropriate data model. There are basically two types of DML: Procedural DML -- require a user to specify what data are needed and how to get those data, like relational algebra. Declarative DML -- require a user to specify what data are needed without specifying how to get those data, like SQL. Declarative DMLs are usually easier to learn and use than are procedural DMLs. The portion of a DML that involves information retrieval is called a query language. SQL Query Language SQL query language is nonprocedural. A query takes as input several tables (possibly only one) and always returns a single table. Example to find all instructors in Comp. Sci. dept 123select namefrom instructorwhere dept_name = 'Comp. Sci.' SQL is NOT a Turing machine equivalent language Database Access from Application Program To be able to compute complex functions SQL is usually embedded in some higher-level language. Non-procedural query languages such as SQL are not as powerful as a universal Turing machine. SQL does not support actions such as input from users, output to displays, or communication over the network, such computations and actions must be written in a host language, such as C/C++, Java or Python, with embedded SQL queries that access the data in the database. Application programs generally access databases through one of Language extensions to allow embedded SQL Application program interface (e.g., ODBC/JDBC) which allow SQL queries to be sent to a database Database Design Logical Design Deciding on the database schema. Database design requires that we find a “good” collection of relation schemas. Business decision – What attributes should we record in the database? Computer Science decision – What relation schemas should we have and how should the attributes be distributed among the various relation schemas? Physical Design Deciding on the physical layout of the database Depend on DBMS platform Database Engine A database system is partitioned into modules that deal with each of the responsibilities of the overall system. The functional components of a database system can be divided into Query processor component Storage manager Transaction management component Storage Manager A program module that provides the interface between the low-level data stored in the database and the application programs and queries submitted to the system. The storage manager is responsible to the following tasks: Interaction with the OS file manager Efficient storing, retrieving and updating of data The storage manager components include: authorization and integrity manager transaction manager file manager buffer manager The storage manager implements several data structures as part of the physical system implementation: Data files -- store the database itself Data dictionary -- stores metadata about the structure of the database, in particular the schema of the database. Indices -- can provide fast access to data items. A database index provides pointers to those data items that hold a particular value. Query Processor The query processor components include: DDL interpreter -- interprets DDL statements and records the definitions in the data dictionary. DML compiler -- translates DML statements in a query language into an evaluation plan consisting of low-level instructions that the query evaluation engine understands. The DML compiler performs query optimization; that is, it picks the lowest cost evaluation plan from among the various alternatives. Query evaluation engine -- executes low-level instructions generated by the DML compiler. Transaction Management A transaction is a collection of operations that performs a single logical function in a database application. Transaction-management component ensures that the database remains in a consistent (correct) state despite system failures (such as power failures and operating system crashes) and transaction failures. Concurrency-control manager controls the interaction among the concurrent transactions, to ensure the consistency of the database. Database Architecture We are now in a position to provide a single picture (Figure 1.5) of the various components of a database system and the connections among them: Database applications are usually partitioned into two or three parts: Two-tier architecture -- the application resides at the client machine, where it invokes database system functionality at the server machine. Three-tier architecture -- the client machine acts as a front end and does not contain any direct database calls. The client end communicates with an application server, usually through a forms interface. The application server in turn communicates with a database system to access data. Database Users and Administrators Database Users 4 categories of users with different purposes and access modes Database Administrator(DBA) A person who has central control over the system is called a database administrator (DBA). Schema definition Storage structure and access-method definition Schema and physical-organization modification Granting of authorization for data access Routine maintenance Periodically backing up the database Ensuring that enough free disk space is available for normal operations, and upgrading disk space as required Monitoring jobs running on the database","link":"/2023/08/28/DBS/DBS_1/"},{"title":"《操作系统概念》第二部分——进程管理","text":"第三章 进程 第四章 线程 第三章 进程","link":"/2023/08/28/OS/OS_Part2/"},{"title":"自制OS之旅1： Bochs 虚拟机","text":"关于Linux环境下Bochs的下载、安装与配置 环境：Ubuntu 22.04.1 LTS Bochs 是一个开源的模拟器，用于模拟x86架构的计算机系统。它允许你在一个虚拟的环境中运行操作系统和应用程序，以便测试、调试和学习计算机系统的运行方式。 下载与安装 Bochs 下载解压完成后 configue-&gt;make-&gt;make install 三步走 12345678910111213141516sudo apt-get -y install gccsudo apt-get -y install build-essentialsudo apt-get -y install g++wget https://udomain.dl.sourceforge.net/project/bochs/bochs/2.6.8/bochs-2.6.8.tar.gztar -zxvf bochs-2.6.8.tar.gzcd bochs-2.6.8./configure \\--prefix=xxx(此处填安装路径，不能有空格) \\--enable-debugger \\--enable-disasm \\--enable-iodebug \\--enable-x86-debugger \\--with-x \\--with-x11makemake install --prefix 用来指定安装目录（绝对路径） --enable-debugger 打开 bochs 自己的调试器 --enable-disasm 使 bochs 支持反汇编 --enable-iodebug 启用 io 接口调试器 --enable-x86-debugger 支持 x86 调试器 --with-x 使用 x windows --with-x11 使用 x11 图形用户接口 如果缺少后两个所需要的库，可以运行 sudo apt-get install libx11-dev xserver-xorg-dev xorg-dev 解决 配置 Bochs 安装完成后，进入自定义的安装目录开写配置文件，配置文件的名字和位置不限。我们可以参考样本文档 share/doc/bochs/bochsrc-sample.txt。 12345678910111213141516171819202122232425# bochsrc.disk: Configue file for Bochs# 指定内存：32 MBmegs: 32# 设置对应真实机器的 BIOS 和 VGA BIOSromimage: file=/YOUR_PATH/bochs/share/bochs/BIOS-bochs-latestvgaromimage: file=/YOUR_PATH/bochs/share/bochs/VGABIOS-lgpl-latest# 设置 Bochs 所使用的磁盘，软盘关键字为 floppy# floppya: 1_44=a.img, status=inserted# 选择启动盘符# boot: floppyboot: disk# 设置日志文件的输出log: bochs.out# 开启或关闭一些功能mouse: enabled=0keyboard: type=mf, serial_delay=250, keymap=/YOUR_PATH/bochs/share/bochs/keymaps/x11-pc-us.map# 硬盘设置ata0: enabled=1, ioaddr1=0x1f0, ioaddr2=0x3f0, irq=14 运行 Bochs 在安装路径下输入 bin/bochs ，默认选项 [2] 回车后输出配置文件名即可成功启动。 创建硬盘 在安装目录下输入如下指令，或者使用 bin/bximage 根据提示创建 1bin/bximage -hd=60M -imgmode=flat -q hd60M.img 然后在 bochsrc.disk 硬盘设置部分加入 1ata0-master: type=disk, path=\"hd60M.img\", mode=flat, cylinders=121, heads=16, spt=63","link":"/2023/08/25/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9C%9F%E7%9B%B8%E8%BF%98%E5%8E%9F/SOS1-bochs/"},{"title":"《操作系统概念》第一部分——概述","text":"第一章 导论 第二章 操作系统结构","link":"/2023/08/25/OS/OS_Part1/"},{"title":"字节前端一面凉经","text":"字节前端一面游记 自我介绍环节，然后开启代码页面 项目拷打 从简历上第一个项目——网页版简易拳皇开始问 概述 如何实现背景框、血条以及人物移动基本功能 css 部分 血条如何实现 早忘了（bushi），后来问 css 会啥我说会 flex，然后考了垂直居中的几种方式（糊出来两种） 这个时候网断了一次，属于是 gg 的开始 js 部分 如何实现人物跳跃功能 凭着记忆写了一个 Player 类，然后设置 gravity 和 v_vertical 属性糊了一个 jump 函数和 render 函数，面试官对细节拷打很久最后给过了。 之后的项目就没继续问了，本来还看了 vue 响应式之类的知识点。 代码部分 js 判断数据类型 typeof 和 isArray 一开始没调对，后来面试官看不下去大发慈悲让我调过了（ js this 相关 首先读代码 1234567891011121314151617class User { constructor(name) { this.name = 'Jarry'; } getInfo() { return { name: 'Tom', getName() { return this.name; } } }}const u1 = new User();u1.getInfo().getName(); 很简单的this问题，问最后输出。然后需要改动 getName 让其输出 Jarry。改箭头函数就行，然后接着问普通函数和箭头函数区别，答了 new args this 仨点但是追问细节没说出来。 反问环节 最后时间到了，问了 如何评价（bushi） 面试官评价基础不够扎实 业务主要内容和技术栈 视频开发，React 如何评价 和想象中不太一样的是没有基础问答环节，所有东西都是 show me your code。没考闭包、事件循环、节流/防抖/柯里化这些 “热点”。 需要对项目极度熟悉，熟悉到可以凭记忆在代码界面复现的程度（被问的项目作于一年前，吃了大亏）。 很考验 js 基础，面试官不认可模棱两可的回答。很可惜只有一天半时间准备面试，临时抱佛脚不太行。","link":"/2023/08/11/%E5%AD%97%E8%8A%82%E5%89%8D%E7%AB%AF%E4%B8%80%E9%9D%A2%E5%87%89%E7%BB%8F/"},{"title":"SWS3004-Lecture 4: Cloud Software Development and Deployment","text":"L4 is about Cloud Software Development and Deployment. Outline Cloud Software Development Learning Objectives SaaS is Different from Traditional Software Different Perspectives of SaaS Development SaaS with Self-managed Infrastructure and Self-managed Platform SaaS with Self-managed Platform SaaS with Self-managed Infrastructure SaaS with Cloud-enabled IaaS and SaaS Summary Cloud Software Development Power of cloud computing – exploits higher service-level abstraction, PaaS and SaaS, to reduce the time and cost of software development SaaS changes the way software is delivered usage-based billing, high scalability, ease of access, automated updates PaaS changes the way the (SaaS) software is developed automates the process of deployment, testing and scaling to reduce manual work and cost of application development Learning Objectives Understand how SaaS applications are different from traditional software/application Understand the different perspectives of SaaS software development SaaS is Different from Traditional Software Pay-per-use - provides web access to commercial software on pay-as you-use vs traditional pay the full license fee Zero infrastructure – customers need not install the software (SaaS developed, deployed and managed by service provider) vs ASP (application service provider) owns and manages dedicated infrastructure for each customers Reduced business cost – 1-many - same SaaS application shared by multiple customers (multi-tenants) vs traditional 1-1 end-users and software relationship Automated updates – Updates performed by service providers (SaaS) not by users (traditional) Suitability of SaaS Not suitable Real-time processing where fast processing of data is needed Organization’s data is more confidential and data localization is needed When on-premise applications fulfil organization’s needs Suitable Consumers require on-demand software rather than full term/licensing-based software Start-up company that cannot invest in buying licensed software Applications with unpredictable and dynamic load Different Perspectives Of SaaS Development 2 key challenges choosing correct multitenancy level(s) - multitenancy can be achieved at different levels such as infrastructure, platform and application governance and security over user data four perspectives SaaS with Self-managed Infrastructure and Self-managed Platform SaaS with Self-managed Platform SaaS with Self-managed Infrastructure SaaS with Cloud-enabled IaaS and PaaS SaaS with Self-managed Infrastructure and Self-managed Platform SaaS with Self-managed Platform SaaS with Self-managed Infrastructure SaaS with Cloud-enabled IaaS and PaaS Summary cloud software development SaaS changes the way software is delivered SaaS is different from traditional software SaaS development: from self-managed to cloud enabled IaaS and/or PaaS cloud-enabled platform services (with advanced capabilities around artificial intelligence (AI), data analytics, blockchain, IoT, …) + server-less computing changes the development, deployment and cost of more complex software applications","link":"/2023/08/10/SWS-3004/SWS3004-Lecture-4/"},{"title":"SWS3004-Lecture 3: Big Data Architecture and Patterns","text":"L3 is about Big Data Architecture and Patterns. Overview Big Data and Big Data Architectures Platforms MapReduce, Hadoop, and HDFS Spark Cloud Dataflow and Beam Big Data and Big Data Architectures Big Data Big Data Architectures Big Data Platform and Applications Multicore/Cluster era → parallel and distributed applications Parallelism Task-parallel Data-parallel Batch (MapReduce, Spark, Cloud Dataflow) Stream (Spark Streaming, Flink, Cloud Dataflow) Mixed System resource demand Compute-intensive Data-intensive Mixed Platforms MapReduce, Hadoop, and HDFS MapReduce Programming Model Supports arbitrarily divisible workload Supports distributed computing on large data sets on multiple machines (clusters, public or private clouds, …) How large an amount of work? Web-scale data on the order of 100s of GBs to TBs or PBs Input data set will not likely fit on a single computer’s hard drive Distributed file system (e.g., Google File System- GFS) is typically required Inspired by the map and reduce functions in functional programming languages Key idea Split data into blocks and assign each block to an instance/process for parallel execution Merge partial results produced by individual instances after all instances completed execution Transform a set of input &lt;key, value&gt; pairs into a set of output &lt;key, value&gt; pairs SPMD (Same Program Multiple Data) - a master instance partitions the data and gathers the partial results Structure of a MapReduce Program Read (a lot of) data MAP (extract data you need from each record) Shuffle and Sort data REDUCE (aggregate, summarize, filter, transform extracted data) Write the results Hadoop Google MapReduce - closed source developed by Google (2004) to process large amounts of raw data Hadoop MapReduce – open source developed by Apache + Yahoo (Java programming language) – a popular implementation of MapReduce Presents MapReduce as an analytics engine with a distributed storage layer referred to as Hadoop Distributed File System (HDFS); HDFS mimics Google File System (GFS) Example: Amazon Elastic MapReduce creates a Hadoop cluster and handles data transfers between Amazon EC2 (computation) and Amazon S3 (storage) Spark Apache Spark Distributed processing framework and programming model for big data workloads Utilizes in-memory caching, and optimized query execution for fast analytic queries against data of any size It provides development APIs in Java, Scala, Python and R, and supports code reuse across multiple workloads: Batch processing Machine Learning Interactive queries Real-time analytics Spark RDD Main abstraction: resilient distributed dataset (RDD) - a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDD - created from the file system or through operations (transformations) Option to persist an RDD in memory or on disk to be reused efficiently across parallel operations RDD used for fault tolerance: recover from node failures Cloud Dataflow and Beam Google Cloud Dataflow “The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing\" reference Apache Beam Parallel computing framework for data processing Multiple SDKs (Java, Python, Go, Scala) Unified data model (PCollection): batch (bounded) streaming (unbounded) Pipelines containing data processing operations (PTransform) Source transforms Processing and Conversion transforms (Count, Sum, Map, GroupBy) Outputting Transforms Used defined transforms Run on multiple execution engines called Runners DirectRunner (Local) Google Dataflow Apache Spark Hadoop Apache Flink","link":"/2023/08/08/SWS-3004/SWS3004-Lecture-3/"},{"title":"SWS3004-Lecture 2: Applications and Paradigms","text":"L2 is about Application and Paradigms of Cloud Computing. Outline Cloud Applications Common Features Applications Challenges in Developing Applications Architectural Styles for Cloud Applications Cloud Application Development Models Characteristics of Cloud Service Models Examples of Setting up a Blog Key Terms Divisible workload Performance isolation Web application architecture layers Application development models (IaaS, PaaS, SaaS) Cloud Applications Common Features of Cloud Providers Basic and higher - level services (IaaS -&gt; SaaS). Compute and storage resources - virtual servers (Linux and Windows) and object store. Deployment management services - load balancer, auto scaling, message queueing, monitoring, …. User interface - graphical user interface, command-line interface. Cloud Applications Focus mainly on enterprise computing. Ideally, an application can partition its workload into n segments and spawn n instances, and the execution time reduced by a factor close to n. (MapReduce) Key Challenges cloud consumer: scale application to accommodate a dynamic load, recover after a system failure, efficiently support checkpoint/restart cloud provider: manage a large number of systems (cloud consumer applications), provides quality of service guarantee Ideal Applications web services database services search services machine learning with massive-scale models Unlikely to perform well applications with a complex workflow and multiple dependencies such as high-performance computing applications with intensive communication among concurrent instances workload cannot be arbitrarily partitioned Challenges in Developing Cloud Applications Performance isolation – how to ensure that customer performance is not affected by other users? Reliability - major concern, server failures expected when a large number of servers cooperate to compute. Cloud infrastructure exhibits latency and bandwidth fluctuations which affect the application performance. Performance considerations limit the amount of data logging (identify unexpected results, errors, monitor application performance). Architectural Styles for Cloud Applications Reliance on Internet and web technology (high accessibility, web browser universality, ease of web-based service development). Cloud services use web technology as both the implementation medium and management interface. 2 basic components of the web are web browser client and web server, i.e., based on client-server architecture. “as-a-Service” Cloud Delivery Models Simple Object Access Protocol (SOAP) : Application protocol for web applications; defines a common web service messaging format for request and response message exchanges; based on the XML, uses TCP or UDP transport protocols. Representational State Transfer (REST): software architecture for distributed hypermedia systems. Supports client communication with stateless servers, platform and language independent, supports data caching, and can be used in the presence of firewalls. Rest is better in simplicity, flexibility, efficiency and scalability, which is suitable for web applications. SOAP is suitable for applications requiring higher security, transaction handling, and reliability, particularly in enterprise-level applications. Cloud Application Development Models Characteristics of Cloud Service Models Infrastructure as a Service (for IT architects) Web access to resources Centralized physical resource management Elastic services and dynamic scaling Shared infrastructure across multiple users Preconfigured VMs Metered services Platform as a Service (for developers) All in one – same IDE to develop, test, deploy, host and maintain applications Web access to development platforms Offline access for developers Built-in scalability Collaborative platform for developers Diverse client tools Software as a Service (for end users) Multi-tenanted applications Web access Centralized management of SaaS services Multi-device support Scalability under varying loads High availability API integration with other software Examples of Setting up a Blog Bonus Track: Function as a Service","link":"/2023/08/03/SWS-3004/SWS3004-Lecture-2/"},{"title":"SWS3004-Lecture 1: Concepts and Models","text":"L1 is about Concepts and Models of Cloud Computing. Outline NIST Definition Cloud Characteristics Cloud Service(Delivery) Models Conceptual Reference Architecture Cloud Deployment Models Summary Key Terms Elasticity On-demand self service Pay-per-use (measured service) Multi-tenancy (location independent resource pooling) Cloud service (delivery) models Cloud deployment models Cloud actors Definition Cloud means \"smooth\" to access, control and measure. It has five essential characteristics, four deployment models and three service models. Cloud Characteristics On-demand self-service through a service portal With cloud computing, you can provision computing services, like server time and network storage, automatically. You won’t need to interact with the service provider. Cloud customers can access their cloud accounts through a web self-service portal to view their cloud services, monitor their usage, and provision and de-provision services. Broad network access (ubiquitous access) Users can access cloud services anytime and anywhere through a terminal device with network connection. Latency and bandwidth both count because they affect the quality of service. Location-independent resource pooling (multi-tenancy) Computing resources are gathered together as pools, like CPU pools, memory pools, etc. With resource pooling, multiple customers can share physical resources using a multi-tenancy model. This model allows customers to share the same applications or infrastructure while maintaining privacy and security. It's a fantastic characteristic of cloud, which abstracts and subdivides physical resources. Rapid elasticity – time to market / fast deployment Cloud services can be elastically provisioned and released, sometimes automatically, so customers can scale quickly based on demand. With rapid and unlimited elasticity of cloud service, you don't need to buy hardware but use cloud resources to satisfy your demand. Measured service (pay-per-use) In cloud systems, a metering capability optimizes resource usage at a level of abstraction appropriate to the type of service. For example, you can use a measured service for storage, processing, bandwidth, and users. Payment is based on actual consumption by the customer via a pay-for-what-you-use model. Monitoring, controlling, and reporting resource use creates a transparent experience for both consumers and providers of the service. Cloud Service(Delivery) Models There are three main models: SaaS, PaaS and IaaS. As for \"steak\" service, IaaS is like providing a kitchen with some pots, PaaS provides raw beef and pepper additionally, and SaaS provides a plate of steak. More convenience, but less space to select. Conceptual Reference Architecture Actor Roles Cloud Consumer - maintains a business relationship with, and uses service from Cloud Providers. Cloud Provider – offers a cloud service to cloud consumers. Cloud Auditor - conducts independent assessment of cloud services, system operations, performance and security of the cloud implementation. Cloud Broker - manages the use, performance and delivery of cloud services, and negotiates relationships between Cloud Providers and Cloud Consumers. Cloud Carrier - provides connectivity and transport of cloud services from Cloud Providers to Cloud Consumers. CLOUD DEPLOYMENT MODELS Private cloud solely for used by an organization for enterprises/corporations with large scale IT Public cloud available to general public, i.e., shared by all consumers open market for on demand computing and IT resources concerns: limited SLA, reliability, availability, security, trust Community cloud shared by several organizations and supporting a specific community Hybrid (federated) cloud two or more public and private clouds that interoperate extends private cloud(s) to include a shared public cloud","link":"/2023/07/28/SWS-3004/SWS3004-Lecture-1/"},{"title":"SWS3004-Lab Exercise 2","text":"This Lab is about Hadoop and Spark, using AWS EMR and S3. Exercise 2.1 Run and compare the execution time of WordCount on Wikipidia’s dump with both Hadoop MapReduce and Spark. You can use either IaaS or PaaS, but make sure you use the same type of setup for both Hadoop and Spark (e.g., if you use EMR for Hadoop MapReduce, then use EMR for Spark also). You have to use the provided input of size 12 GB. Is there any difference in the programming model and ease of programming? Is there any difference in performance? Please explain it in maximum 3 paragraphs. You can include up to 2 performance plots. Input dataset address on AWS S3: s3://sws3004-2023/input/enwiki-12GB.xml You must use this input dataset for both Hadoop MapReduce and Spark. (Tip: use the entire address s3://sws3004-2023/input/enwiki-12GB.xml as parameter to your MapReduce job) Part 1. Hadoop Mapreduce First I create a S3 bucket and upload files: Then I create an EMR cluster, select the S3 bucket created in the previous step as my S3 folder, and select m4.large and default 3 instances as the instance configuration. Now add a step, using WordCount.jar to process the input from s3://sws3004-2023/input/enwiki-12GB.xml. In my S3 bucket, I can check the output when the step finished. Hadoop MapReduce performance: The process takes 42 minutes totally. Part 2. Spark I clear the S3 bucket and upload the files of Spark: Then create a new EMR for Spark, and add a step. Spark performance: The process takes 16 minutes totally. It seems that Spark is better than Hadoop in performance. The reason is Hadoop uses disk to store data while Spark uses memory to store data, which can reduce the I/O time. Also, MapReduce requires a lot of time to sort during Shuffle, and sorting seems inevitable in MapReduce's Shuffle. When Spark is in Shuffle, sorting is only required for some situations, which is faster. Is there any difference in the programming model and ease of programming? Hadoop MapReduce: Hadoop MapReduce is a programming model designed for distributed data processing on large clusters of commodity hardware. The MapReduce programming model has two steps to process our data: Map and Reduce. Map: In the Map phase, the input data is divided into splits, and each split is processed independently by multiple mapper tasks in parallel. The mapper tasks extract key-value pairs from the input data and emit intermediate key-value pairs. Shuffle and Sort: The intermediate key-value pairs emitted by the mappers are shuffled and sorted based on the keys. This step ensures that all values for the same key are grouped together and sent to the same reducer task. Reduce: In the Reduce phase, the sorted and shuffled intermediate data is processed by reducer tasks. Each reducer task processes a subset of the intermediate data, grouped by keys. The reducer tasks aggregate the values associated with each key and produce the final output. Spark: Spark is a better distributed data processing engine, which extends the MapReduce model and offers more versatility and performance improvements. Spark introduces the concept of Resilient Distributed Datasets (RDDs), which are the fundamental data abstraction in Spark. RDDs are distributed collections of data that can be processed in parallel. Spark provides a more general programming model compared to Hadoop MapReduce. It supports not only Map and Reduce operations but also various other transformations and actions on RDDs, such as filter, join, groupByKey, reduceByKey and so on. Additionally, Spark offers specialized libraries like Spark SQL for structured data processing, Spark Streaming for real-time data streaming, and MLlib for machine learning tasks. Difference in Programming Model and Ease of Programming: I found a comparison form in Lecture slides: Programming Model: Hadoop MapReduce has a more rigid programming model, where data is processed in two distinct phases (Map and Reduce), and users need to explicitly handle intermediate data shuffle and sort. Spark provides a more flexible and expressive programming model with RDDs, allowing users to perform complex operations on distributed data through a wide range of transformations and actions. Ease of Programming: Spark generally offers better ease of programming due to its high-level APIs and expressive transformations and actions on RDDs. It simplifies the development of distributed data processing applications, and its concise syntax often leads to shorter and more readable code compared to Hadoop MapReduce. Hadoop MapReduce, being more low-level, might require developers to write additional code for tasks like intermediate data serialization and deserialization, which can make the development process more cumbersome. In summary, Spark provides a more powerful and user-friendly programming model compared to Hadoop MapReduce. Spark's RDDs and higher-level APIs make it easier for developers to write distributed data processing applications, leading to faster development cycles and more efficient data processing. Exercise 2.2 Write and run on AWS EMR a MapReduce program that computes the total number of followers and followees for each user in a Twitter dataset. The dataset is provided to you in the file twitter_combined.txt taken from http://snap.stanford.edu/data/egonets-Twitter.html. Each line of this file contains two user ids A and B meaning “User A follows User B”. For example, the first line is “214328887 34428380” and it means that “User 214328887 follows User 34428380”. My code is below: 123456789101112131415161718192021222324252627282930// here is the mappublic static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { public void map(LongWritable key, Text texts, Context context) throws IOException, InterruptedException { String[] users = texts.toString().split(\" \"); // split as user0 user1 IntWritable followers = new IntWritable(-1); // negative IntWritable follows = new IntWritable(1); // positive context.write(new Text(users[1]), followers); context.write(new Text(users[0]), follows); }}// here is the reducepublic static class Reduce extends Reducer&lt;Text, IntWritable, Text, Text&gt; { public void reduce(Text texts, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int follows = 0; int followers = 0; for (IntWritable text : texts) { if (text.get() &gt; 0) { follows += text.get(); } else { followers -= text.get(); } } // output the result for each user context.write(texts, new Text(String.format(\"Followers %d\", inDegree))); context.write(texts, new Text(String.format(\"Follows %d\", outDegree))); }} Then we create a S3 and upload the files: Now we can add a step in EMR and view the final output: So User 214328887 has 628 followers, and follows 951 users.","link":"/2023/07/28/SWS-3004/AWS-Lab2/"},{"title":"SWS3004-Lab Exercise 1","text":"This Lab is based on Amazon Web Services(AWS)，including EC2, Lambda, SQS and CloudWatch. Exercise 1.1 Passwordless SSH access between two EC2 instances Create two (2) AWS EC2 instances (virtual machines) with a Linux-based operating system(e.g., Ubuntu) and set up passwordless SSH access among them. Let’s suppose we name those 2 instances A and B. Passwordless SSH means that we can SSH into instance A from instance B and vice versa without being asked for a password. How do you do that? Please explain it in 1-2 paragraphs. [3 marks] Firstly, I create two EC2 instances(serverA and serverB) with a Ubuntu-20.04 OS. The details are shown in Figure 1 and Figure 2 below. The IPv4 of serverA is 54.174.141.190, and the IPv4 of server B is 44.207.230.223. Let's start with password-less access from A to B. First, type the command ssh-keygen -t rsa in instance A. This command will generate a pair of public/private keys in the ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa, shown in Figure 3. I met a trouble here: I use command ssh-copy-id -i ~/.ssh/id_rsa.pub ubuntu@44.207.230.223 to copy the public key of A to the authorized_keys of B, but it shows Permission denied. The reason for this error is this command will overwrite the file authorized_keys of B, but this file is already exists so this command was denied. Instead, I use cat ~/.ssh/id_rsa.pub in instance A to display my public key, and use echo \"MY_PUBLIC_KEY\" &gt;&gt; ~/.ssh/authorized_keys in instance B to paste the key. Now type ssh ubuntu@44.207.230.223 in instance A, we can access instance B successfully, without any password! View relevant screenshot in Figure 4. Symmetrically, to make instance B to access to A without any password, just generate ssh-key pair in instance B, and follow the steps above once again. In Figure 5, both A and B successfully access to each other, without password. Finally, I summarized the theory of password-less SSH in figure 6. Password-less SSH using a different port Similar to Exercise 1.1., create two (2) AWS EC2 instances (virtual machines) with a Linux based operating system (e.g., Ubuntu) and setup passwordless SSH access among them but this time use a different port for the SSH server (change the default port 22 to port 2222). Do you need to make any other modifications to your EC2 instances? [4 marks] Let's continue on the basis of Exercise 1.1. We have achieved password-less SSH access in the default port 22, and now we need to achieve it in port 2222. First, I edit the security groups of instance A and B, adding a new rule to allow 2222 port, just like Figure 7 below. Then I add Port 2222 in /etc/ssh/sshd_config, shown in Figure 8. However, I failed for the first time. How could be? The key is not to forget to restart the service. Type sudo service ssh restart to restart service, and then do password-less SSH access. In figure 9, Instance A and B can password-less access to each other using Port 2222. Exercise 1.2 Start: Hello World! Start by creating and running a Python AWS hello world using AWS Management Console, as shown during Lecture 2. Take screenshots of your Lambda function, test event and the log output to show that the program runs successfully. Pay attention to setting the role of the Lambda function to “LabRole”. [2 marks] I create and run a Python AWS hello world using AWS Management Console, the screenshots of lambda function, test event and the log output are shown in Figure 10/11/12. Create an AWS SQS Create an AWS Simple Queue Service (SQS) Queue and take a screenshot of the created queue. After creating the queue, note down its Amazon Resource Name (ARN). [2 marks] Here we create a AWS SQS Queue shown in Figure 13. The ARN is arn:aws:sqs:us-east-1:368136098362:Queue1 Change the code of Lambda Function Change the code of your Lambda function such that it returns the received message from SQS. You are allowed to search on the Internet for how to do that. Please include the code in your submission(report). Next, add the created queue (identified by its ARN) as trigger for the Lambda function. Take a screenshot of the “Function overview Info” section of your Lambda function. [2 marks] Now we need to change the code of the Lambda function such that it returns the received message from SQS. The code is shown in the block below. 12345678import jsonprint('Loading function')def lambda_handler(event, context): print('Received message: %s' % event['Records'][0]['body']) message = event['Records'][0]['body'] return message Then we need to add the created queue (identified by its ARN) as trigger for the Lambda function. There are two ways here: The first way is add the trigger in AWS Interface, shown in Figure 14. The second way is to use command line interface, shown in Figure 15. Two queues are successfully added as trigger, shown in Figure 16. The Function Overview is shown in Figure 17. Send and Receive Message In the SQS dashboard of your queue, click “Send and receive message”, then send a message with the body “Hello from SWS3004!”. Message Group ID and Message deduplication ID can be set to 0. Press “Send message”. Next, go to CloudWatch -&gt; Logs -&gt; Log groups and find the logs for your Lambda function. Click on the relevant log stream (e.g., the latest). There should be a message “Hello from SWS3004!” somewhere in this log stream. Take a screenshot and include it in the report. [2 marks] We send a message hello:-) from my SQS queue1, and finds the logs for my function hello, shown as Figure 18 and Figure 19.","link":"/2023/07/15/SWS-3004/AWS-Lab1/"},{"title":"谈北邮计语工作改进","text":"在北邮计语融媒体中心平台运营部工作两年，一年当部员一年当副部。临别回想工作得失，不免诸多遗憾，在此一表。 北邮计语平台运营部是什么？ 北邮计语公众号本质上是北邮计院的官方媒体，由北邮计语融媒体中心平台运营部负责推送的制作、发布与日常运营。 官方媒体这一属性决定了两点： 平台运营部的工作流程中，审核或者“领导”的话语权最大。 平台推送的核心内容是党政主题，占据推送内容的一半左右。 平台运营部直接管理公众号平台，因而需要与其他各部门的宣传工作对接。 谈一些问题 部门的纵向工作流程 上级领导 &gt; 指导老师 &gt; 部长团 &gt; 组长 &gt; 部员 向上负责 权力只对权力的来源负责。在这样一级级下压的金字塔建筑中，由上层向下层下达指令，从顶层设计逐渐到底层实现，从抽象逐渐到具体。上层给予下层一些必要的权力，当然也拥有对工作的审核与否定权。 流程漫长 让我们从《力量》栏目的制作流程谈起。首先领导决定要制作一个党政类品牌节目，然后 指导老师会和部长团开始设计并做出一版demo，最终敲定这个栏目的框架与模板，并让下面的小组进行每期的制作。每期制作好的推送都会由部员交给组长再交给部长团，然后交给老师审核；老师将审核意见下传给部长团，部长团自己进行修改或者再下发给部员。 这种审核流程会让工作效率极慢——一项工作往往由上到下逐渐具体化，交回审核者时会被细致地审核，并一级一级传递给下层改正。这样的审核常常持续1~2天，工作内容会被反复地进行细节修改乃至推倒重来。沟通环节的长度与频率极大延长了工作周期，当然也给负责修改的下层带来极大的折磨。 横向对接与内部工作 部门横向对接上，其他部门常常无视书面要求，压着截止时间进行申请对接。我们部门有着明确的接稿时间与格式标准，但需要常常“破例”。 我部内部工作架构是部长-&gt;副部长-&gt;组长-&gt;部员，副部长也兼任着组长一职。由部长负责分配任务给组长，组长下发给组员。部长与副部长同时负责送审、运营及各项其他事务。这样出现的问题是，某种意义上副部长只是一个拥有额外任务的“组长plus”，不具有部门范围的调度权力。 当部长这一环停滞时，其他人没有分配调度工作的权力，进而导致工作停摆。停摆到逾期时又需要迅速补救，导致工作压力增大的同时工作质量下降。 部门没有工作计划表，很多工作会被忽视或者遗忘，需要在临近截止时赶工。比如最基本的二十四节气与节日推送，完全可以写在工作表上提前去做，但我们并没有这样一张工作表。 问题所在——制度 诚然，很多问题都可以简单归咎于人的因素，归咎于人在能力与性格上的局限性。但根本所在我想还是在制度。认真制定并严格执行的规章制度是一个团队战斗力和凝聚力的关键所在，好的制度可以合理规划工作流程，界定各人权力与职责，进而增加团队的工作效率与内在凝聚力。 从推送制作着手优化 对部员定期组织系统的培训交流会，将工作规范以书面形式着重强调，避免低级错误的产生可以大大减轻审核压力。 部长放权至部长团 分配调度的权力由部长扩大至部长团，破除按单个职能划定分工的方式（xx负责排版、xx负责文案这种划定方式不符合工作实际，也不利于工作效率），保证工作流程不会因为某个人的缺失而停滞。 决策层下调 决策有着天然向上集中的属性，所以制度设计中应该着力避免一切决策都往上堆的问题，更何况决策的执行往往由下层实施，这样的沟通成本很大。 最终拍板决策的权力在审核老师，部长团自己对推送的修改意见往往会与之相左甚至大相径庭，部长团没有动力在老师之前进行预先审核。 工作量过大，一项推送前前后后涉及到的步骤与细节相对繁复，更何况推送频次少则每周数个多则每天数个，决策向上集中一个是效率很依赖审核老师的状态，一个是底下部长团乃至部员有时苦不堪言。 因此我认为将审核的权利与责任下放给全体部长团，部长团直接与制作小组沟通，审核老师做给予充分信任与最后的微调即可。 Farewell 总归到了告别的时候，我曾经以为自己会在北邮计语一直工作到毕业，但最后还是选择了离开。一段辛苦的旅程结束总会给我一种解脱感，但这次并没有太大触动。可能是前面还有很多路要走，或是我曾经真的爱得深沉。 回想过去两年，常常因为部门工作案牍劳形——在假期、期末甚至除夕夜，在酒店、餐厅乃至火车上，我都有打开电脑处理工作的经历。一路走来，我自认为圆满完成了几乎所有负责的任务，只是可惜亲手设计的两个栏目一个被一拖再拖，一个已经与我没有关系（也许总有一天会与读者见面）。 有太多事情的成败并非自己能决定，它们日后的进程又只有天知道。所以尽力就好，享受接下来的旅程。","link":"/2023/07/05/%E8%B0%88%E5%8C%97%E9%82%AE%E8%AE%A1%E8%AF%AD%E5%B7%A5%E4%BD%9C%E6%94%B9%E8%BF%9B/"},{"title":"虎符前端工程实践2","text":"如何限制段落最大显示字数 Vue 实现限制段落字数 原版代码及显示： 12345&lt;div class=\"item-warn-board\"&gt; &lt;div class=\"row_spacing\" v-if=\"AlertList.length &gt; 0\" v-for=\"i in 5\" style=\"text-align: left;margin-bottom: 8px\"&gt; {{ AlertList[i].annotations.summary }} &lt;/div&gt;&lt;/div&gt; 我们需要将每段告警限制在一行之内，并将多余字符以 ... 替换 Vue 实现 在 methods 中添加如下方法： 123456truncateText(text, maxLength) { if (text.length &gt; maxLength) { return text.slice(0, maxLength) + '...' } return text}, 再在 template 中将页面改成： 123456&lt;div class=\"item-warn-board\"&gt; &lt;div class=\"row_spacing\" v-if=\"AlertList.length &gt; 0\" v-for=\"i in 5\" style=\"text-align: left;margin-bottom: 10px\"&gt; &lt;span class=\"bullet\"&gt;&lt;/span&gt; {{ truncateText(AlertList[i].annotations.summary, 47) }} &lt;/div&gt;&lt;/div&gt; 这样，我们便成功地实现限制文本长度的功能：","link":"/2023/06/09/hufu/%E8%99%8E%E7%AC%A6%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B52/"},{"title":"虎符前端工程实践1","text":"深入 echarts 悬浮提示窗 Vue 页面设置切屏选项 深入 echarts 悬浮提示窗 在用 echarts 生成图表的时候，我们可以设置 tooltip 属性生成悬浮窗，通过鼠标悬浮展示当前位置的详细数据。例如，这段代码可以生成如下悬浮窗： 1234567tooltip: { trigger: 'axis', axisPointer: { type: 'cross' }, padding: [5, 10]}, trigger：触发类型。 'axis'： 指定触发提示窗的方式为在坐标轴上的轴线上触发，也就是当鼠标悬停在图表上的时候显示提示窗。 axisPointer：坐标轴指示器配置项，功能优先级低于轴上的 axisPointer。 type: 'cross'：指定提示窗的指示器类型为交叉指示器，交叉指示器会在X轴和Y轴上都显示一个十字准星，方便用户对鼠标所在位置的数据进行查看。 但简单生成的悬浮提示框往往没有单位，导致信息不够直观完善。我通过 tooltip 的 formatter 选项进行代码改进： 123456789101112131415161718192021222324252627tooltip: { trigger: 'axis', axisPointer: { type: 'cross' }, padding: [5, 10], formatter: function(params) { let tooltipContent = '' params.forEach(function(item) { const seriesName = item.seriesName const value = item.value let unit = '' // assign unit based on series name if (seriesName === 'CPU占用率' || seriesName === '内存占用率' || seriesName === '磁盘占用率') { unit = '%' } else if (seriesName === '网络流量流入' || seriesName === '网络流量流出') { unit = 'Mb/s' } // add unit to data tooltipContent += seriesName + ': ' + value + ' ' + unit + '&lt;br&gt;' }) return tooltipContent }}, Vue 页面设置切屏选项 12345678910111213141516&lt;template&gt; ... &lt;el-row type=\"flex\" justify=\"space-between\" class=\"row_spacing\"&gt; &lt;el-col :span=\"4\"&gt; &lt;el-radio-group v-model=\"radio\" @change=\"changeArea\"&gt; &lt;el-radio-button id=\"1\" :label=\"$t('config')\" /&gt; &lt;el-radio-button id=\"2\" :label=\"$t('status')\" /&gt; &lt;/el-radio-group&gt; &lt;/el-col&gt; &lt;el-col :span=\"20\" class=\"clearfix\"&gt; // 已经存在的其他按钮 &lt;/el-col&gt; &lt;/el-row&gt; ...&lt;/template&gt; 这里用到了 element-ui，el-col 默认分为 24 份，我给需要添加的两个按钮（配置和状态）分了 ”4“ 的空间。 v-model=\"radio\" 表示将 radio 变量与 &lt;el-radio-group&gt; 组件的选中状态进行双向绑定。当选中状态发生变化时，radio 变量的值也会相应更新 @change=\"changeArea\" 则是监听 &lt;el-radio-group&gt; 的 change 事件，并在事件触发时调用名为 changeArea 的方法 $t 是 Vue I18n 提供的一个函数，用于在 Vue 组件中获取翻译文本。通过 $t('config')，你可以获取名为 'config' 的翻译文本 接着我们在 method 区域实现 changeArea，总体思路是根据 radio 控制两个段落的显示 12345678910111213141516changeArea() { const config = document.getElementById('config') const status = document.getElementById('status') switch (this.radio) { case this.$t('config'): config.style.display = 'block' status.style.display = 'none' break case this.$t('status'): this.showStatus = 'true' config.style.display = 'none' status.style.display = 'block' break }}, 同时，我还希望点进页面的时候默认显示配置内容，那么只需要在 data 中加入 12345data() { return { radio: this.$t('config'), }}, Bug 问题一在于，初始的时候不会调用 changeArea，我很痛苦的发现两个段落同时工作了（现在发现不会出现，应该是个未定义的bug）。 方案一：我在 mounted 生命周期中调用 changeArea 函数，在组件被挂载之后调用函数显然可以修复这一问题。 问题二在于，采用方案一后状态对应的图表不再加载，除非我改变页面大小。这个问题很诡异，迫使我另辟蹊径去解决问题一。 修复 自始至终的问题在状态这一选项上，初始化时它作为不速之客蹦出来，改正后又姗姗不来。所以我在 data 中添加了 showStatus 这一参数并初始化为 false，并在状态段落添加 v-if=\"showStatus\" 属性。在 changeArea 触发 status 条件后才会置为 true。这样，就完美控制了状态段落的显示。","link":"/2023/06/06/hufu/%E8%99%8E%E7%AC%A6%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B51/"}],"tags":[{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"杂谈","slug":"杂谈","link":"/tags/%E6%9D%82%E8%B0%88/"},{"name":"思考","slug":"思考","link":"/tags/%E6%80%9D%E8%80%83/"},{"name":"Database","slug":"Database","link":"/tags/Database/"},{"name":"OS","slug":"OS","link":"/tags/OS/"},{"name":"Cloud","slug":"Cloud","link":"/tags/Cloud/"},{"name":"Lab","slug":"Lab","link":"/tags/Lab/"},{"name":"虎符","slug":"虎符","link":"/tags/%E8%99%8E%E7%AC%A6/"},{"name":"VUE","slug":"VUE","link":"/tags/VUE/"},{"name":"echarts","slug":"echarts","link":"/tags/echarts/"}],"categories":[{"name":"面试","slug":"面试","link":"/categories/%E9%9D%A2%E8%AF%95/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"},{"name":"Database","slug":"Database","link":"/categories/Database/"},{"name":"OS","slug":"OS","link":"/categories/OS/"},{"name":"云开发","slug":"云开发","link":"/categories/%E4%BA%91%E5%BC%80%E5%8F%91/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"","text":"关于本博客 写作对我来说是一件爱恨交织的事情。我喜欢将自己的所思所感诉诸文字，却又往往囿于斟词酌句和重构文章结构中无法自拔。但我想这件事情总归是值得坚持的，它可以让我更加严肃的去审视、思考并总结，以安静的文字沉淀下来。这是我想要的生活方式吧，见字如面。 我的博客主要分为以下三个部分： 成于思：严肃思考 勤于学：知识学习 敏于行：工程实践 歌于途：生活随感 关于我 北京邮电大学计算机学院本科生 目前兴趣在全栈开发 共产主义者","link":"/about/index.html"},{"title":"life","text":"","link":"/life/index.html"},{"title":"","text":"勤于学 云计算（Cloud Computing with Big Data) NUS 暑校课程，包括 4 个 Lecture 和 2 个 Lab Lecture Note Concepts and Models Applications and Paradigms Big Data Architecture and Patterns Cloud Software Development and Deployment Lab Report Lab Exercise 1 Lab Exercise 2","link":"/study/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"","text":"敏于行：工程实践 虎符系列 “虎符”是徐鹏老师的实验室项目，我担任一些开发任务。 虎符前端工程实践1 虎符前端工程实践2 虎符前端工程实践3","link":"/tech/index.html"},{"title":"","text":"","link":"/think/index.html"}]}