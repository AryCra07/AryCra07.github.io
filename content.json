{"posts":[{"title":"字节前端一面凉经","text":"字节前端一面游记 自我介绍环节，然后开启代码页面 项目拷打 从简历上第一个项目——网页版简易拳皇开始问 概述 如何实现背景框、血条以及人物移动基本功能 css 部分 血条如何实现 早忘了（bushi），后来问 css 会啥我说会 flex，然后考了垂直居中的几种方式（糊出来两种） 这个时候网断了一次，属于是 gg 的开始 js 部分 如何实现人物跳跃功能 凭着记忆写了一个 Player 类，然后设置 gravity 和 v_vertical 属性糊了一个 jump 函数和 render 函数，面试官对细节拷打很久最后给过了。 之后的项目就没继续问了，本来还看了 vue 响应式之类的知识点。 代码部分 js 判断数据类型 typeof 和 isArray 一开始没调对，后来面试官看不下去大发慈悲让我调过了（ js this 相关 首先读代码 1234567891011121314151617class User { constructor(name) { this.name = name; } getInfo() { return { name: 'Tom', getName() { return this.name; } } }}const u1 = new User();u1.getInfo().getName(); 很简单的this问题，问最后输出。然后需要改动 getName 让其输出 Jarry。改箭头函数就行，然后接着问普通函数和箭头函数区别，答了 new args this 仨点但是追问细节没说出来。 反问环节 最后时间到了，问了 如何评价（bushi） 面试官评价基础不够扎实 业务主要内容和技术栈 视频开发，React 如何评价 和想象中不太一样的是没有基础问答环节，所有东西都是 show me your code。没考闭包、事件循环、节流/防抖/柯里化这些 “热点”。 需要对项目极度熟悉，熟悉到可以凭记忆在代码界面复现的程度（被问的项目作于一年前，吃了大亏）。 很考验 js 基础，面试官不认可模棱两可的回答。很可惜只有一天半时间准备面试，临时抱佛脚不太行。","link":"/2023/08/11/%E5%AD%97%E8%8A%82%E5%89%8D%E7%AB%AF%E4%B8%80%E9%9D%A2%E5%87%89%E7%BB%8F/"},{"title":"SWS3004-Lecture 1: Concepts and Models","text":"L1 is Concepts and Models of Cloud Computing. Outline NIST Definition Cloud Characteristics Cloud Service(Delivery) Models Conceptual Reference Architecture Cloud Deployment Models Summary Key Terms Elasticity On-demand self service Pay-per-use (measured service) Multi-tenancy (location independent resource pooling) Cloud service (delivery) models Cloud deployment models Cloud actors Definition Cloud means \"smooth\" to access, control and measure. It has five essential characteristics, four deployment models and three service models. Cloud Characteristics On-demand self-service through a service portal With cloud computing, you can provision computing services, like server time and network storage, automatically. You won’t need to interact with the service provider. Cloud customers can access their cloud accounts through a web self-service portal to view their cloud services, monitor their usage, and provision and de-provision services. Broad network access (ubiquitous access) Users can access cloud services anytime and anywhere through a terminal device with network connection. Latency and bandwidth both count because they affect the quality of service. Location-independent resource pooling (multi-tenancy) Computing resources are gathered together as pools, like CPU pools, memory pools, etc. With resource pooling, multiple customers can share physical resources using a multi-tenancy model. This model allows customers to share the same applications or infrastructure while maintaining privacy and security. It's a fantastic characteristic of cloud, which abstracts and subdivides physical resources. Rapid elasticity – time to market / fast deployment Cloud services can be elastically provisioned and released, sometimes automatically, so customers can scale quickly based on demand. With rapid and unlimited elasticity of cloud service, you don't need to buy hardware but use cloud resources to satisfy your demand. Measured service (pay-per-use) In cloud systems, a metering capability optimizes resource usage at a level of abstraction appropriate to the type of service. For example, you can use a measured service for storage, processing, bandwidth, and users. Payment is based on actual consumption by the customer via a pay-for-what-you-use model. Monitoring, controlling, and reporting resource use creates a transparent experience for both consumers and providers of the service. Cloud Service(Delivery) Models There are three main models: SaaS, PaaS and IaaS. As for \"steak\" service, IaaS is like providing a kitchen with some pots, PaaS provides raw beef and pepper additionally, and SaaS provides a plate of steak. More convenience, but less space to select. Conceptual Reference Architecture Actor Roles Cloud Consumer - maintains a business relationship with, and uses service from Cloud Providers. Cloud Provider – offers a cloud service to cloud consumers. Cloud Auditor - conducts independent assessment of cloud services, system operations, performance and security of the cloud implementation. Cloud Broker - manages the use, performance and delivery of cloud services, and negotiates relationships between Cloud Providers and Cloud Consumers. Cloud Carrier - provides connectivity and transport of cloud services from Cloud Providers to Cloud Consumers. CLOUD DEPLOYMENT MODELS Private cloud solely for used by an organization for enterprises/corporations with large scale IT Public cloud available to general public, i.e., shared by all consumers open market for on demand computing and IT resources concerns: limited SLA, reliability, availability, security, trust Community cloud shared by several organizations and supporting a specific community Hybrid (federated) cloud two or more public and private clouds that interoperate extends private cloud(s) to include a shared public cloud","link":"/2023/07/28/SWS3004-Lecture-1/"},{"title":"SWS3004-Lab Exercise 2","text":"This Lab is about Hadoop and Spark, using AWS EMR and S3. Exercise 2.1 Run and compare the execution time of WordCount on Wikipidia’s dump with both Hadoop MapReduce and Spark. You can use either IaaS or PaaS, but make sure you use the same type of setup for both Hadoop and Spark (e.g., if you use EMR for Hadoop MapReduce, then use EMR for Spark also). You have to use the provided input of size 12 GB. Is there any difference in the programming model and ease of programming? Is there any difference in performance? Please explain it in maximum 3 paragraphs. You can include up to 2 performance plots. Input dataset address on AWS S3: s3://sws3004-2023/input/enwiki-12GB.xml You must use this input dataset for both Hadoop MapReduce and Spark. (Tip: use the entire address s3://sws3004-2023/input/enwiki-12GB.xml as parameter to your MapReduce job) Part 1. Hadoop Mapreduce First I create a S3 bucket and upload files: Then I create an EMR cluster, select the S3 bucket created in the previous step as my S3 folder, and select m4.large and default 3 instances as the instance configuration. Now add a step, using WordCount.jar to process the input from s3://sws3004-2023/input/enwiki-12GB.xml. In my S3 bucket, I can check the output when the step finished. Hadoop MapReduce performance: The process takes 42 minutes totally. Part 2. Spark I clear the S3 bucket and upload the files of Spark: Then create a new EMR for Spark, and add a step. Spark performance: The process takes 16 minutes totally. It seems that Spark is better than Hadoop in performance. The reason is Hadoop uses disk to store data while Spark uses memory to store data, which can reduce the I/O time. Also, MapReduce requires a lot of time to sort during Shuffle, and sorting seems inevitable in MapReduce's Shuffle. When Spark is in Shuffle, sorting is only required for some situations, which is faster. Is there any difference in the programming model and ease of programming? Hadoop MapReduce: Hadoop MapReduce is a programming model designed for distributed data processing on large clusters of commodity hardware. The MapReduce programming model has two steps to process our data: Map and Reduce. Map: In the Map phase, the input data is divided into splits, and each split is processed independently by multiple mapper tasks in parallel. The mapper tasks extract key-value pairs from the input data and emit intermediate key-value pairs. Shuffle and Sort: The intermediate key-value pairs emitted by the mappers are shuffled and sorted based on the keys. This step ensures that all values for the same key are grouped together and sent to the same reducer task. Reduce: In the Reduce phase, the sorted and shuffled intermediate data is processed by reducer tasks. Each reducer task processes a subset of the intermediate data, grouped by keys. The reducer tasks aggregate the values associated with each key and produce the final output. Spark: Spark is a better distributed data processing engine, which extends the MapReduce model and offers more versatility and performance improvements. Spark introduces the concept of Resilient Distributed Datasets (RDDs), which are the fundamental data abstraction in Spark. RDDs are distributed collections of data that can be processed in parallel. Spark provides a more general programming model compared to Hadoop MapReduce. It supports not only Map and Reduce operations but also various other transformations and actions on RDDs, such as filter, join, groupByKey, reduceByKey and so on. Additionally, Spark offers specialized libraries like Spark SQL for structured data processing, Spark Streaming for real-time data streaming, and MLlib for machine learning tasks. Difference in Programming Model and Ease of Programming: I found a comparison form in Lecture slides: Programming Model: Hadoop MapReduce has a more rigid programming model, where data is processed in two distinct phases (Map and Reduce), and users need to explicitly handle intermediate data shuffle and sort. Spark provides a more flexible and expressive programming model with RDDs, allowing users to perform complex operations on distributed data through a wide range of transformations and actions. Ease of Programming: Spark generally offers better ease of programming due to its high-level APIs and expressive transformations and actions on RDDs. It simplifies the development of distributed data processing applications, and its concise syntax often leads to shorter and more readable code compared to Hadoop MapReduce. Hadoop MapReduce, being more low-level, might require developers to write additional code for tasks like intermediate data serialization and deserialization, which can make the development process more cumbersome. In summary, Spark provides a more powerful and user-friendly programming model compared to Hadoop MapReduce. Spark's RDDs and higher-level APIs make it easier for developers to write distributed data processing applications, leading to faster development cycles and more efficient data processing. Exercise 2.2 Write and run on AWS EMR a MapReduce program that computes the total number of followers and followees for each user in a Twitter dataset. The dataset is provided to you in the file twitter_combined.txt taken from http://snap.stanford.edu/data/egonets-Twitter.html. Each line of this file contains two user ids A and B meaning “User A follows User B”. For example, the first line is “214328887 34428380” and it means that “User 214328887 follows User 34428380”. My code is below: 123456789101112131415161718192021222324252627282930// here is the mappublic static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; { public void map(LongWritable key, Text texts, Context context) throws IOException, InterruptedException { String[] users = texts.toString().split(\" \"); // split as user0 user1 IntWritable followers = new IntWritable(-1); // negative IntWritable follows = new IntWritable(1); // positive context.write(new Text(users[1]), followers); context.write(new Text(users[0]), follows); }}// here is the reducepublic static class Reduce extends Reducer&lt;Text, IntWritable, Text, Text&gt; { public void reduce(Text texts, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException { int follows = 0; int followers = 0; for (IntWritable text : texts) { if (text.get() &gt; 0) { follows += text.get(); } else { followers -= text.get(); } } // output the result for each user context.write(texts, new Text(String.format(\"Followers %d\", inDegree))); context.write(texts, new Text(String.format(\"Follows %d\", outDegree))); }} Then we create a S3 and upload the files: Now we can add a step in EMR and view the final output: So User 214328887 has 628 followers, and follows 951 users.","link":"/2023/07/28/AWS-Lab2/"},{"title":"SWS3004-Lab Exercise 1","text":"This Lab is based on Amazon Web Services(AWS)，including EC2, Lambda, SQS and CloudWatch. Exercise 1.1 Passwordless SSH access between two EC2 instances Create two (2) AWS EC2 instances (virtual machines) with a Linux-based operating system(e.g., Ubuntu) and set up passwordless SSH access among them. Let’s suppose we name those 2 instances A and B. Passwordless SSH means that we can SSH into instance A from instance B and vice versa without being asked for a password. How do you do that? Please explain it in 1-2 paragraphs. [3 marks] Firstly, I create two EC2 instances(serverA and serverB) with a Ubuntu-20.04 OS. The details are shown in Figure 1 and Figure 2 below. The IPv4 of serverA is 54.174.141.190, and the IPv4 of server B is 44.207.230.223. Let's start with password-less access from A to B. First, type the command ssh-keygen -t rsa in instance A. This command will generate a pair of public/private keys in the ~/.ssh/id_rsa.pub and ~/.ssh/id_rsa, shown in Figure 3. I met a trouble here: I use command ssh-copy-id -i ~/.ssh/id_rsa.pub ubuntu@44.207.230.223 to copy the public key of A to the authorized_keys of B, but it shows Permission denied. The reason for this error is this command will overwrite the file authorized_keys of B, but this file is already exists so this command was denied. Instead, I use cat ~/.ssh/id_rsa.pub in instance A to display my public key, and use echo \"MY_PUBLIC_KEY\" &gt;&gt; ~/.ssh/authorized_keys in instance B to paste the key. Now type ssh ubuntu@44.207.230.223 in instance A, we can access instance B successfully, without any password! View relevant screenshot in Figure 4. Symmetrically, to make instance B to access to A without any password, just generate ssh-key pair in instance B, and follow the steps above once again. In Figure 5, both A and B successfully access to each other, without password. Finally, I summarized the theory of password-less SSH in figure 6. Password-less SSH using a different port Similar to Exercise 1.1., create two (2) AWS EC2 instances (virtual machines) with a Linux based operating system (e.g., Ubuntu) and setup passwordless SSH access among them but this time use a different port for the SSH server (change the default port 22 to port 2222). Do you need to make any other modifications to your EC2 instances? [4 marks] Let's continue on the basis of Exercise 1.1. We have achieved password-less SSH access in the default port 22, and now we need to achieve it in port 2222. First, I edit the security groups of instance A and B, adding a new rule to allow 2222 port, just like Figure 7 below. Then I add Port 2222 in /etc/ssh/sshd_config, shown in Figure 8. However, I failed for the first time. How could be? The key is not to forget to restart the service. Type sudo service ssh restart to restart service, and then do password-less SSH access. In figure 9, Instance A and B can password-less access to each other using Port 2222. Exercise 1.2 Start: Hello World! Start by creating and running a Python AWS hello world using AWS Management Console, as shown during Lecture 2. Take screenshots of your Lambda function, test event and the log output to show that the program runs successfully. Pay attention to setting the role of the Lambda function to “LabRole”. [2 marks] I create and run a Python AWS hello world using AWS Management Console, the screenshots of lambda function, test event and the log output are shown in Figure 10/11/12. Create an AWS SQS Create an AWS Simple Queue Service (SQS) Queue and take a screenshot of the created queue. After creating the queue, note down its Amazon Resource Name (ARN). [2 marks] Here we create a AWS SQS Queue shown in Figure 13. The ARN is arn:aws:sqs:us-east-1:368136098362:Queue1 Change the code of Lambda Function Change the code of your Lambda function such that it returns the received message from SQS. You are allowed to search on the Internet for how to do that. Please include the code in your submission(report). Next, add the created queue (identified by its ARN) as trigger for the Lambda function. Take a screenshot of the “Function overview Info” section of your Lambda function. [2 marks] Now we need to change the code of the Lambda function such that it returns the received message from SQS. The code is shown in the block below. 12345678import jsonprint('Loading function')def lambda_handler(event, context): print('Received message: %s' % event['Records'][0]['body']) message = event['Records'][0]['body'] return message Then we need to add the created queue (identified by its ARN) as trigger for the Lambda function. There are two ways here: The first way is add the trigger in AWS Interface, shown in Figure 14. The second way is to use command line interface, shown in Figure 15. Two queues are successfully added as trigger, shown in Figure 16. The Function Overview is shown in Figure 17. Send and Receive Message In the SQS dashboard of your queue, click “Send and receive message”, then send a message with the body “Hello from SWS3004!”. Message Group ID and Message deduplication ID can be set to 0. Press “Send message”. Next, go to CloudWatch -&gt; Logs -&gt; Log groups and find the logs for your Lambda function. Click on the relevant log stream (e.g., the latest). There should be a message “Hello from SWS3004!” somewhere in this log stream. Take a screenshot and include it in the report. [2 marks] We send a message hello:-) from my SQS queue1, and finds the logs for my function hello, shown as Figure 18 and Figure 19.","link":"/2023/07/15/AWS-Lab1/"},{"title":"谈北邮计语工作改进","text":"在北邮计语融媒体中心平台运营部工作两年，一年当部员一年当副部。临别回想工作得失，不免诸多遗憾，在此一表。 北邮计语平台运营部是什么？ 北邮计语公众号本质上是北邮计院的官方媒体，由北邮计语融媒体中心平台运营部负责推送的制作、发布与日常运营。 官方媒体这一属性决定了两点： 平台运营部的工作流程中，审核或者“领导”的话语权最大。 平台推送的核心内容是党政主题，占据推送内容的一半左右。 平台运营部直接管理公众号平台，因而需要与其他各部门的宣传工作对接。 谈一些问题 部门的纵向工作流程 上级领导 &gt; 指导老师 &gt; 部长团 &gt; 组长 &gt; 部员 向上负责 权力只对权力的来源负责。在这样一级级下压的金字塔建筑中，由上层向下层下达指令，从顶层设计逐渐到底层实现，从抽象逐渐到具体。上层给予下层一些必要的权力，当然也拥有对工作的审核与否定权。 流程漫长 让我们从《力量》栏目的制作流程谈起。首先领导决定要制作一个党政类品牌节目，然后 指导老师会和部长团开始设计并做出一版demo，最终敲定这个栏目的框架与模板，并让下面的小组进行每期的制作。每期制作好的推送都会由部员交给组长再交给部长团，然后交给老师审核；老师将审核意见下传给部长团，部长团自己进行修改或者再下发给部员。 这种审核流程会让工作效率极慢——一项工作往往由上到下逐渐具体化，交回审核者时会被细致地审核，并一级一级传递给下层改正。这样的审核常常持续1~2天，工作内容会被反复地进行细节修改乃至推倒重来。沟通环节的长度与频率极大延长了工作周期，当然也给负责修改的下层带来极大的折磨。 横向对接与内部工作 部门横向对接上，其他部门常常无视书面要求，压着截止时间进行申请对接。我们部门有着明确的接稿时间与格式标准，但需要常常“破例”。 我部内部工作架构是部长-&gt;副部长-&gt;组长-&gt;部员，副部长也兼任着组长一职。由部长负责分配任务给组长，组长下发给组员。部长与副部长同时负责送审、运营及各项其他事务。这样出现的问题是，某种意义上副部长只是一个拥有额外任务的“组长plus”，不具有部门范围的调度权力。 当部长这一环停滞时，其他人没有分配调度工作的权力，进而导致工作停摆。停摆到逾期时又需要迅速补救，导致工作压力增大的同时工作质量下降。 部门没有工作计划表，很多工作会被忽视或者遗忘，需要在临近截止时赶工。比如最基本的二十四节气与节日推送，完全可以写在工作表上提前去做，但我们并没有这样一张工作表。 问题所在——制度 诚然，很多问题都可以简单归咎于人的因素，归咎于人在能力与性格上的局限性。但根本所在我想还是在制度。认真制定并严格执行的规章制度是一个团队战斗力和凝聚力的关键所在，好的制度可以合理规划工作流程，界定各人权力与职责，进而增加团队的工作效率与内在凝聚力。 从推送制作着手优化 对部员定期组织系统的培训交流会，将工作规范以书面形式着重强调，避免低级错误的产生可以大大减轻审核压力。 部长放权至部长团 分配调度的权力由部长扩大至部长团，破除按单个职能划定分工的方式（xx负责排版、xx负责文案这种划定方式不符合工作实际，也不利于工作效率），保证工作流程不会因为某个人的缺失而停滞。 决策层下调 决策有着天然向上集中的属性，所以制度设计中应该着力避免一切决策都往上堆的问题，更何况决策的执行往往由下层实施，这样的沟通成本很大。 最终拍板决策的权力在审核老师，部长团自己对推送的修改意见往往会与之相左甚至大相径庭，部长团没有动力在老师之前进行预先审核。 工作量过大，一项推送前前后后涉及到的步骤与细节相对繁复，更何况推送频次少则每周数个多则每天数个，决策向上集中一个是效率很依赖审核老师的状态，一个是底下部长团乃至部员有时苦不堪言。 因此我认为将审核的权利与责任下放给全体部长团，部长团直接与制作小组沟通，审核老师做给予充分信任与最后的微调即可。 Farewell 总归到了告别的时候，我曾经以为自己会在北邮计语一直工作到毕业，但最后还是选择了离开。一段辛苦的旅程结束总会给我一种解脱感，但这次并没有太大触动。可能是前面还有很多路要走，或是我曾经真的爱得深沉。 回想过去两年，常常因为部门工作案牍劳形——在假期、期末甚至除夕夜，在酒店、餐厅乃至火车上，我都有打开电脑处理工作的经历。一路走来，我自认为圆满完成了几乎所有负责的任务，只是可惜亲手设计的两个栏目一个被一拖再拖，一个已经与我没有关系（也许总有一天会与读者见面）。 有太多事情的成败并非自己能决定，它们日后的进程又只有天知道。所以尽力就好，享受接下来的旅程。","link":"/2023/07/05/%E8%B0%88%E5%8C%97%E9%82%AE%E8%AE%A1%E8%AF%AD%E5%B7%A5%E4%BD%9C%E6%94%B9%E8%BF%9B/"},{"title":"虎符前端工程实践2","text":"如何限制段落最大显示字数 Vue 实现限制段落字数 原版代码及显示： 12345&lt;div class=\"item-warn-board\"&gt; &lt;div class=\"row_spacing\" v-if=\"AlertList.length &gt; 0\" v-for=\"i in 5\" style=\"text-align: left;margin-bottom: 8px\"&gt; {{ AlertList[i].annotations.summary }} &lt;/div&gt;&lt;/div&gt; 我们需要将每段告警限制在一行之内，并将多余字符以 ... 替换 Vue 实现 在 methods 中添加如下方法： 123456truncateText(text, maxLength) { if (text.length &gt; maxLength) { return text.slice(0, maxLength) + '...' } return text}, 再在 template 中将页面改成： 123456&lt;div class=\"item-warn-board\"&gt; &lt;div class=\"row_spacing\" v-if=\"AlertList.length &gt; 0\" v-for=\"i in 5\" style=\"text-align: left;margin-bottom: 10px\"&gt; &lt;span class=\"bullet\"&gt;&lt;/span&gt; {{ truncateText(AlertList[i].annotations.summary, 47) }} &lt;/div&gt;&lt;/div&gt; 这样，我们便成功地实现限制文本长度的功能：","link":"/2023/06/09/%E8%99%8E%E7%AC%A6%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B52/"},{"title":"虎符前端工程实践1","text":"深入 echarts 悬浮提示窗 Vue 页面设置切屏选项 深入 echarts 悬浮提示窗 在用 echarts 生成图表的时候，我们可以设置 tooltip 属性生成悬浮窗，通过鼠标悬浮展示当前位置的详细数据。例如，这段代码可以生成如下悬浮窗： 1234567tooltip: { trigger: 'axis', axisPointer: { type: 'cross' }, padding: [5, 10]}, trigger：触发类型。 'axis'： 指定触发提示窗的方式为在坐标轴上的轴线上触发，也就是当鼠标悬停在图表上的时候显示提示窗。 axisPointer：坐标轴指示器配置项，功能优先级低于轴上的 axisPointer。 type: 'cross'：指定提示窗的指示器类型为交叉指示器，交叉指示器会在X轴和Y轴上都显示一个十字准星，方便用户对鼠标所在位置的数据进行查看。 但简单生成的悬浮提示框往往没有单位，导致信息不够直观完善。我通过 tooltip 的 formatter 选项进行代码改进： 123456789101112131415161718192021222324252627tooltip: { trigger: 'axis', axisPointer: { type: 'cross' }, padding: [5, 10], formatter: function(params) { let tooltipContent = '' params.forEach(function(item) { const seriesName = item.seriesName const value = item.value let unit = '' // assign unit based on series name if (seriesName === 'CPU占用率' || seriesName === '内存占用率' || seriesName === '磁盘占用率') { unit = '%' } else if (seriesName === '网络流量流入' || seriesName === '网络流量流出') { unit = 'Mb/s' } // add unit to data tooltipContent += seriesName + ': ' + value + ' ' + unit + '&lt;br&gt;' }) return tooltipContent }}, Vue 页面设置切屏选项 12345678910111213141516&lt;template&gt; ... &lt;el-row type=\"flex\" justify=\"space-between\" class=\"row_spacing\"&gt; &lt;el-col :span=\"4\"&gt; &lt;el-radio-group v-model=\"radio\" @change=\"changeArea\"&gt; &lt;el-radio-button id=\"1\" :label=\"$t('config')\" /&gt; &lt;el-radio-button id=\"2\" :label=\"$t('status')\" /&gt; &lt;/el-radio-group&gt; &lt;/el-col&gt; &lt;el-col :span=\"20\" class=\"clearfix\"&gt; // 已经存在的其他按钮 &lt;/el-col&gt; &lt;/el-row&gt; ...&lt;/template&gt; 这里用到了 element-ui，el-col 默认分为 24 份，我给需要添加的两个按钮（配置和状态）分了 ”4“ 的空间。 v-model=\"radio\" 表示将 radio 变量与 &lt;el-radio-group&gt; 组件的选中状态进行双向绑定。当选中状态发生变化时，radio 变量的值也会相应更新 @change=\"changeArea\" 则是监听 &lt;el-radio-group&gt; 的 change 事件，并在事件触发时调用名为 changeArea 的方法 $t 是 Vue I18n 提供的一个函数，用于在 Vue 组件中获取翻译文本。通过 $t('config')，你可以获取名为 'config' 的翻译文本 接着我们在 method 区域实现 changeArea，总体思路是根据 radio 控制两个段落的显示 12345678910111213141516changeArea() { const config = document.getElementById('config') const status = document.getElementById('status') switch (this.radio) { case this.$t('config'): config.style.display = 'block' status.style.display = 'none' break case this.$t('status'): this.showStatus = 'true' config.style.display = 'none' status.style.display = 'block' break }}, 同时，我还希望点进页面的时候默认显示配置内容，那么只需要在 data 中加入 12345data() { return { radio: this.$t('config'), }}, Bug 问题一在于，初始的时候不会调用 changeArea，我很痛苦的发现两个段落同时工作了（现在发现不会出现，应该是个未定义的bug）。 方案一：我在 mounted 生命周期中调用 changeArea 函数，在组件被挂载之后调用函数显然可以修复这一问题。 问题二在于，采用方案一后状态对应的图表不再加载，除非我改变页面大小。这个问题很诡异，迫使我另辟蹊径去解决问题一。 修复 自始至终的问题在状态这一选项上，初始化时它作为不速之客蹦出来，改正后又姗姗不来。所以我在 data 中添加了 showStatus 这一参数并初始化为 false，并在状态段落添加 v-if=\"showStatus\" 属性。在 changeArea 触发 status 条件后才会置为 true。这样，就完美控制了状态段落的显示。","link":"/2023/06/06/%E8%99%8E%E7%AC%A6%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B51/"}],"tags":[{"name":"Cloud","slug":"Cloud","link":"/tags/Cloud/"},{"name":"Lab","slug":"Lab","link":"/tags/Lab/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"面试","slug":"面试","link":"/tags/%E9%9D%A2%E8%AF%95/"},{"name":"虎符","slug":"虎符","link":"/tags/%E8%99%8E%E7%AC%A6/"},{"name":"VUE","slug":"VUE","link":"/tags/VUE/"},{"name":"echarts","slug":"echarts","link":"/tags/echarts/"},{"name":"杂谈","slug":"杂谈","link":"/tags/%E6%9D%82%E8%B0%88/"},{"name":"思考","slug":"思考","link":"/tags/%E6%80%9D%E8%80%83/"}],"categories":[{"name":"云开发","slug":"云开发","link":"/categories/%E4%BA%91%E5%BC%80%E5%8F%91/"},{"name":"面试","slug":"面试","link":"/categories/%E9%9D%A2%E8%AF%95/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"随笔","slug":"随笔","link":"/categories/%E9%9A%8F%E7%AC%94/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"","text":"关于本博客 写作对我来说是一件爱恨交织的事情。我喜欢将自己的所思所感诉诸文字，却又往往囿于斟词酌句和重构文章结构中无法自拔。但我想这件事情总归是值得坚持的，它可以让我更加严肃的去审视、思考并总结，以安静的文字沉淀下来。这是我想要的生活方式吧，见字如面。 我的博客主要分为以下三个部分： 成于思：严肃思考 勤于学：知识学习 敏于行：工程实践 歌于途：生活随感 关于我 北京邮电大学计算机学院本科生 主要兴趣在全栈开发","link":"/about/index.html"},{"title":"life","text":"","link":"/life/index.html"},{"title":"","text":"勤于学 云计算（Cloud Computing with Big Data) NUS 暑校课程，包括 4 个 Lecture 和 2 个 Lab Lecture Note Concepts and Models Applications and Paradigms Big Data Architecture and Patterns Cloud Software Development Lab Report Lab Exercise 1 Lab Exercise 2","link":"/study/index.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"","text":"敏于行：工程实践 虎符系列 “虎符”是徐鹏老师的云实验室项目，我忝列其中担任一些前端任务。 虎符前端工程实践1","link":"/tech/index.html"},{"title":"","text":"","link":"/think/index.html"}]}